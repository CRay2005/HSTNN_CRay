import torch
import torch.nn as nn
from collections import OrderedDict
from utils.kfac_utils import fetch_mat_weights
# from utils.common_utils import (tensor_to_list, PresetLRScheduler)
from utils.prune_utils import (filter_indices,
                               filter_indices_ni,
                               get_threshold,
                               update_indices,
                               normalize_factors,
                               prune_model_ni)
# from utils.network_utils import stablize_bn
from tqdm import tqdm

from hessian_fact import get_trace_hut
from pyhessian.hessian import hessian
from pyhessian.utils import group_product, group_add, normalization, get_params_grad, hessian_vector_product, orthnormal

import numpy as np
import time
import scipy.linalg
import os.path
from os import path

# æ·»åŠ ç”¨äºé‡è¦æ€§åˆ†å¸ƒåˆ†æçš„å¯¼å…¥
from sklearn.cluster import KMeans

class HessianPruner:
        def __init__(self,
                     model,
                     trace_file_name,
                     fix_layers=0,
                     hessian_mode='trace'):
            self.iter = 0
            #self.known_modules = {'Linear', 'Conv2d'}
            self.modules = []
            self.model = model
            self.fix_layers = fix_layers

            self.W_pruned = {}
            self.S_l = None

            self.hessian_mode = hessian_mode
            self.trace_file_name = trace_file_name

            self.importances = {}
            self._inversed = False
            self._cfgs = {}
            self._indices = {}
            
        # top function
        def make_pruned_model(self, dataloader, criterion, device, snn_ratio, seed, batch_size, bptt, ntokens, is_loader=False, normalize=True, re_init=False, n_v=300):
            self.snn_ratio = snn_ratio # use for some special case, particularly slq_full, slq_layer
            self.seed = seed
            self._prepare_model()
            self.mask_list = self._compute_hessian_importance(dataloader, criterion, device, batch_size, bptt, ntokens,  is_loader, n_v=n_v)
            print("Finished Hessian Importance Computation!")
            return self.mask_list

        def _prepare_model(self):
            count = 0
            for it in self.model.named_parameters():
                if it[0].find("all_fc") >= 0:
                    continue
                if it[0].find("wh") >= 0:
                    continue
                if it[0].find("decoder") >= 0:
                    continue
                if it[0].find("fv") >= 0:
                    continue
                if it[0].find("encoder") >= 0:
                    continue
                self.modules.append(it)

        def _analyze_importance_distribution(self, sorted_list, method='percentile', network_type='RNN', target_retention=0.3):
            """
            åˆ†æé‡è¦æ€§åˆ†å¸ƒå¹¶è®¾å®šé˜ˆå€¼
            Args:
                sorted_list: æ’åºåçš„é‡è¦æ€§åˆ—è¡¨ [(index, importance), ...]
                method: åˆ†ææ–¹æ³•
                network_type: ç½‘ç»œç±»å‹ï¼Œç”¨äºæ‰“å°ä¿¡æ¯
                target_retention: ç›®æ ‡ä¿ç•™ç‡ï¼ˆ0-1ä¹‹é—´ï¼‰
            Returns:
                threshold: é˜ˆå€¼
                analysis_results: åˆ†æç»“æœå­—å…¸
            """
            importances = [item[1] for item in sorted_list]
            importances_array = np.array(importances)
            
            analysis_results = {
                'importances': importances_array,
                'max': np.max(importances_array),
                'min': np.min(importances_array),
                'mean': np.mean(importances_array),
                'std': np.std(importances_array),
                'method': method,
                'network_type': network_type,
                'target_retention': target_retention
            }
            
            print(f"\n{network_type} Importance Distribution Statistics:")
            print(f"  Max: {analysis_results['max']:.6f}")
            print(f"  Min: {analysis_results['min']:.6f}")
            print(f"  Mean: {analysis_results['mean']:.6f}")
            print(f"  Std: {analysis_results['std']:.6f}")
            
            if method == 'percentile':
                # æ–¹æ³•1: ç™¾åˆ†ä½æ•°æ–¹æ³• - ç›´æ¥æ ¹æ®ç›®æ ‡ä¿ç•™ç‡è®¾å®šé˜ˆå€¼
                percentile = (1 - target_retention) * 100
                threshold = np.percentile(importances_array, percentile)
                analysis_results['threshold_info'] = f"{percentile:.1f}th percentile"
                
            elif method == 'adaptive_statistical':
                # æ–¹æ³•2: è‡ªé€‚åº”ç»Ÿè®¡æ–¹æ³• - è‡ªåŠ¨è°ƒæ•´ç³»æ•°ä»¥è¾¾åˆ°ç›®æ ‡ä¿ç•™ç‡
                mean_val = analysis_results['mean']
                std_val = analysis_results['std']
                
                # äºŒåˆ†æœç´¢æ‰¾åˆ°åˆé€‚çš„ç³»æ•°kï¼Œä½¿å¾—ä¿ç•™ç‡æ¥è¿‘ç›®æ ‡å€¼
                def get_retention_rate(k):
                    test_threshold = mean_val + k * std_val
                    return np.sum(importances_array >= test_threshold) / len(importances_array)
                
                # äºŒåˆ†æœç´¢èŒƒå›´
                k_low, k_high = -5.0, 5.0
                tolerance = 0.02  # å…è®¸2%çš„è¯¯å·®
                
                for _ in range(50):  # æœ€å¤šè¿­ä»£50æ¬¡
                    k_mid = (k_low + k_high) / 2
                    current_rate = get_retention_rate(k_mid)
                    
                    if abs(current_rate - target_retention) < tolerance:
                        break
                    elif current_rate > target_retention:
                        k_low = k_mid
                    else:
                        k_high = k_mid
                
                threshold = mean_val + k_mid * std_val
                final_rate = get_retention_rate(k_mid)
                analysis_results['threshold_info'] = f"Mean+{k_mid:.2f}*Std (achieved {final_rate:.1%})"
                analysis_results['adaptive_k'] = k_mid
                
            elif method == 'improved_clustering':
                # æ–¹æ³•3: æ”¹è¿›çš„èšç±»æ–¹æ³• - ä½¿ç”¨æ›´å¤šèšç±»å¹¶é€‰æ‹©åˆé€‚çš„é˜ˆå€¼
                if len(importances) >= 6:
                    # å°è¯•3ä¸ªèšç±»
                    kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)
                    clusters = kmeans.fit_predict(importances_array.reshape(-1, 1))
                    centers = sorted(kmeans.cluster_centers_.flatten(), reverse=True)
                    
                    # é€‰æ‹©èƒ½å¤Ÿè¾¾åˆ°ç›®æ ‡ä¿ç•™ç‡çš„é˜ˆå€¼
                    # å°è¯•ä¸åŒçš„é˜ˆå€¼ç‚¹ï¼šæœ€é«˜èšç±»ä¸­å¿ƒã€ä¸­é—´èšç±»ä¸­å¿ƒã€èšç±»ä¸­å¿ƒé—´çš„ä¸­ç‚¹
                    candidate_thresholds = [
                        centers[0],  # æœ€é«˜èšç±»ä¸­å¿ƒ
                        centers[1],  # ä¸­é—´èšç±»ä¸­å¿ƒ
                        (centers[0] + centers[1]) / 2,  # æœ€é«˜å’Œä¸­é—´çš„ä¸­ç‚¹
                        (centers[1] + centers[2]) / 2   # ä¸­é—´å’Œæœ€ä½çš„ä¸­ç‚¹
                    ]
                    
                    best_threshold = candidate_thresholds[0]
                    best_diff = float('inf')
                    
                    for candidate in candidate_thresholds:
                        retention = np.sum(importances_array >= candidate) / len(importances_array)
                        diff = abs(retention - target_retention)
                        if diff < best_diff:
                            best_diff = diff
                            best_threshold = candidate
                    
                    threshold = best_threshold
                    final_retention = np.sum(importances_array >= threshold) / len(importances_array)
                    analysis_results['threshold_info'] = f"3-cluster optimization (achieved {final_retention:.1%})"
                    analysis_results['cluster_centers'] = centers
                    analysis_results['clusters'] = clusters
                else:
                    threshold = np.percentile(importances_array, (1-target_retention)*100)
                    analysis_results['threshold_info'] = f"Insufficient data, using percentile"
                    
            elif method == 'top_k':
                # æ–¹æ³•4: Top-Kæ–¹æ³• - ç›´æ¥é€‰æ‹©å‰Kä¸ªé‡è¦çš„ç¥ç»å…ƒ
                k = int(len(importances) * target_retention)
                threshold = importances_array[k-1] if k > 0 else importances_array[0]
                analysis_results['threshold_info'] = f"Top-{k} selection"
                analysis_results['selected_k'] = k
                
            elif method == 'top_k_direct':
                # æ–¹æ³•4b: ç›´æ¥Top-Kæ–¹æ³• - æ‚¨æåˆ°çš„ç®€å•æ–¹æ¡ˆ
                # è¿™æ˜¯æœ€ç›´æ¥çš„æ–¹æ³•ï¼šç›´æ¥æŒ‰æ¯”ä¾‹æˆªå–é™åºåºåˆ—çš„å‰Nä¸ªå…ƒç´ 
                k = max(1, int(len(importances) * target_retention))
                sorted_importances = np.sort(importances_array)[::-1]  # é™åºæ’åˆ—
                threshold = sorted_importances[k-1] if k <= len(sorted_importances) else sorted_importances[-1]
                
                actual_retention = k / len(importances)
                selected_quality = sorted_importances[:k].sum() / importances_array.sum()
                
                print(f"    ğŸ“Š Direct method selected {k}/{len(importances)} neurons")
                print(f"    ğŸ“ˆ Retention ratio: {actual_retention:.2%} (exactly as target {target_retention:.2%})")
                print(f"    ğŸ¯ Quality achieved: {selected_quality:.2%}")
                
                analysis_results['threshold_info'] = f"Direct top-{k} selection (quality={selected_quality:.2%})"
                analysis_results['selected_k'] = k
                analysis_results['direct_quality'] = selected_quality
                analysis_results['note'] = "This is the simple approach you mentioned - directly taking top N elements"
                
            elif method == 'pareto_optimal':
                # æ–¹æ³•5: å¸•ç´¯æ‰˜æœ€ä¼˜æ–¹æ³•
                # çœŸæ­£çš„å¤šç›®æ ‡ä¼˜åŒ–ï¼šå¯»æ‰¾é‡è¦æ€§åˆ†å¸ƒçš„è‡ªç„¶æ–­ç‚¹
                efficiencies = []
                qualities = []
                marginal_benefits = []  # è¾¹é™…æ•ˆç›Š
                
                # è®¡ç®—ä¸åŒé˜ˆå€¼ä¸‹çš„æŒ‡æ ‡
                for percentile in range(5, 96, 5):  # æ›´ç»†ç²’åº¦çš„æœç´¢
                    t = np.percentile(importances_array, 100-percentile)
                    selected = importances_array >= t
                    
                    if selected.sum() > 0:
                        efficiency = selected.sum() / len(selected)  # ä¿ç•™æ¯”ä¾‹
                        quality = importances_array[selected].sum() / importances_array.sum()  # é‡è¦æ€§è´¨é‡å æ¯”
                        
                        # è®¡ç®—è¾¹é™…æ•ˆç›Šï¼šæ¯å¢åŠ 1%ä¿ç•™ç‡å¸¦æ¥çš„è´¨é‡æå‡
                        if len(qualities) > 0:
                            marginal_benefit = (quality - qualities[-1]) / max(0.01, efficiency - efficiencies[-1])
                        else:
                            marginal_benefit = quality / efficiency if efficiency > 0 else 0
                        
                        efficiencies.append(efficiency)
                        qualities.append(quality)
                        marginal_benefits.append(marginal_benefit)
                
                efficiencies = np.array(efficiencies)
                qualities = np.array(qualities)
                marginal_benefits = np.array(marginal_benefits)
                
                # ç­–ç•¥1: å¯»æ‰¾è¾¹é™…æ•ˆç›Šæ˜¾è‘—ä¸‹é™çš„æ‹ç‚¹ï¼ˆè†ç‚¹æ£€æµ‹ï¼‰
                if len(marginal_benefits) >= 3:
                    # è®¡ç®—è¾¹é™…æ•ˆç›Šçš„äºŒé˜¶å·®åˆ†ï¼Œå¯»æ‰¾æ€¥å‰§ä¸‹é™ç‚¹
                    diff2 = np.diff(marginal_benefits, n=2)
                    if len(diff2) > 0:
                        knee_candidates = np.where(diff2 < -np.std(diff2))[0]
                        if len(knee_candidates) > 0:
                            knee_idx = knee_candidates[0] + 2  # è°ƒæ•´ç´¢å¼•
                        else:
                            knee_idx = len(efficiencies) // 2
                    else:
                        knee_idx = len(efficiencies) // 2
                else:
                    knee_idx = 0
                
                # ç­–ç•¥2: å¸•ç´¯æ‰˜å‰æ²¿åˆ†æ
                # å¯»æ‰¾æ•ˆç‡-è´¨é‡æ›²çº¿ä¸Šçš„å¸•ç´¯æ‰˜æœ€ä¼˜ç‚¹
                pareto_indices = []
                for i in range(len(efficiencies)):
                    is_pareto = True
                    for j in range(len(efficiencies)):
                        if i != j:
                            # å¦‚æœå­˜åœ¨å…¶ä»–ç‚¹åœ¨æ•ˆç‡å’Œè´¨é‡ä¸Šéƒ½å ä¼˜ï¼Œåˆ™å½“å‰ç‚¹ä¸æ˜¯å¸•ç´¯æ‰˜æœ€ä¼˜
                            if (efficiencies[j] >= efficiencies[i] and qualities[j] >= qualities[i] and
                                (efficiencies[j] > efficiencies[i] or qualities[j] > qualities[i])):
                                is_pareto = False
                                break
                    if is_pareto:
                        pareto_indices.append(i)
                
                # ç­–ç•¥3: ç»¼åˆå†³ç­–
                if len(pareto_indices) > 0:
                    # åœ¨å¸•ç´¯æ‰˜å‰æ²¿ä¸Šé€‰æ‹©æœ€æ¥è¿‘ç›®æ ‡çš„ç‚¹
                    pareto_efficiencies = efficiencies[pareto_indices]
                    pareto_qualities = qualities[pareto_indices]
                    
                    # å¦‚æœæŒ‡å®šäº†target_retentionï¼Œåœ¨å¸•ç´¯æ‰˜å‰æ²¿ä¸Šå¯»æ‰¾æœ€æ¥è¿‘çš„ç‚¹
                    if target_retention is not None:
                        distances = np.abs(pareto_efficiencies - target_retention)
                        best_pareto_idx = pareto_indices[np.argmin(distances)]
                    else:
                        # å¦åˆ™é€‰æ‹©è¾¹é™…æ•ˆç›Šæœ€é«˜çš„å¸•ç´¯æ‰˜ç‚¹
                        pareto_marginal = marginal_benefits[pareto_indices]
                        best_pareto_idx = pareto_indices[np.argmax(pareto_marginal)]
                    
                    best_idx = best_pareto_idx
                    strategy_used = "Pareto-optimal selection"
                else:
                    # å¦‚æœæ²¡æœ‰æ˜æ˜¾çš„å¸•ç´¯æ‰˜å‰æ²¿ï¼Œä½¿ç”¨è†ç‚¹
                    best_idx = knee_idx
                    strategy_used = "Knee-point detection"
                
                # ç¡®ä¿ç´¢å¼•æœ‰æ•ˆ
                best_idx = max(0, min(best_idx, len(efficiencies) - 1))
                
                # è®¡ç®—æœ€ç»ˆé˜ˆå€¼
                final_efficiency = efficiencies[best_idx]
                final_quality = qualities[best_idx]
                
                # é‡æ–°è®¡ç®—å¯¹åº”çš„é˜ˆå€¼
                target_count = max(1, int(len(importances_array) * final_efficiency))
                sorted_importances = np.sort(importances_array)[::-1]  # é™åºæ’åˆ—
                if target_count <= len(sorted_importances):
                    threshold = sorted_importances[target_count - 1]
                else:
                    threshold = np.min(importances_array)
                
                # æ‰“å°æˆªå–æ¯”ä¾‹ä¿¡æ¯
                actual_retention_ratio = target_count / len(importances_array)
                print(f"    ğŸ“Š Pareto method selected {target_count}/{len(importances_array)} neurons")
                print(f"    ğŸ“ˆ Actual retention ratio: {actual_retention_ratio:.2%} (target was {target_retention:.2%})")
                print(f"    ğŸ¯ Quality achieved: {final_quality:.2%}")
                print(f"    ğŸ”§ Strategy used: {strategy_used}")
                
                analysis_results['threshold_info'] = f"{strategy_used}: efficiency={final_efficiency:.2%}, quality={final_quality:.2%}"
                analysis_results['pareto_efficiencies'] = efficiencies
                analysis_results['pareto_qualities'] = qualities
                analysis_results['marginal_benefits'] = marginal_benefits
                analysis_results['pareto_indices'] = pareto_indices
                analysis_results['best_idx'] = best_idx
                analysis_results['strategy_used'] = strategy_used
                analysis_results['final_efficiency'] = final_efficiency
                analysis_results['final_quality'] = final_quality
            
            elif method == 'ema_threshold':
                # æ–¹æ³•6: æŒ‡æ•°ç§»åŠ¨å¹³å‡é˜ˆå€¼
                # è®¡ç®—é‡è¦æ€§çš„æŒ‡æ•°ç§»åŠ¨å¹³å‡ï¼Œç”¨äºå¹³æ»‘å¤„ç†
                alpha = 0.1  # å¹³æ»‘å› å­
                ema = importances_array[0]
                ema_values = [ema]
                
                for i in range(1, len(importances_array)):
                    ema = alpha * importances_array[i] + (1 - alpha) * ema
                    ema_values.append(ema)
                
                # ä½¿ç”¨EMAå€¼çš„ç™¾åˆ†ä½æ•°ä½œä¸ºé˜ˆå€¼
                ema_array = np.array(ema_values)
                percentile = (1 - target_retention) * 100
                threshold = np.percentile(ema_array, percentile)
                analysis_results['threshold_info'] = f"EMA-based {percentile:.1f}th percentile"
                analysis_results['ema_values'] = ema_array
                
            else:
                # é»˜è®¤ä½¿ç”¨ç™¾åˆ†ä½æ•°æ–¹æ³•
                percentile = (1 - target_retention) * 100
                threshold = np.percentile(importances_array, percentile)
                analysis_results['threshold_info'] = f"Default percentile method"
            
            analysis_results['threshold'] = threshold
            
            # è®¡ç®—å®é™…ä¿ç•™ç‡
            actual_retention = np.sum(importances_array >= threshold) / len(importances_array)
            analysis_results['actual_retention'] = actual_retention
            
            print(f"  {method} method threshold: {threshold:.6f} ({analysis_results['threshold_info']})")
            print(f"  Actual retention rate: {actual_retention:.1%}")
            
            return threshold, analysis_results

        def _global_pareto_allocation(self, global_sorted, max_retain_neurons, rnn_total, snn_total):
            """
            å…¨å±€å¸•ç´¯æ‰˜é¢„ç®—åˆ†é…æ–¹æ³•
            Args:
                global_sorted: å…¨å±€æ’åºçš„ç¥ç»å…ƒåˆ—è¡¨ [(idx, importance, type, layer_size), ...]
                max_retain_neurons: æœ€å¤§ä¿ç•™ç¥ç»å…ƒæ•°é‡
                rnn_total: RNNç¥ç»å…ƒæ€»æ•°
                snn_total: SNNç¥ç»å…ƒæ€»æ•°
            Returns:
                (rnn_budget, snn_budget): RNNå’ŒSNNçš„é¢„ç®—åˆ†é…
            """
            print(f"    ğŸ§® Computing global Pareto allocation...")
            
            # ç­–ç•¥1: åˆ†æå‰Nä¸ªæœ€é‡è¦ç¥ç»å…ƒçš„ç±»å‹åˆ†å¸ƒ
            top_neurons = global_sorted[:max_retain_neurons]
            rnn_count_in_top = sum(1 for neuron in top_neurons if neuron[2] == 'RNN')
            snn_count_in_top = sum(1 for neuron in top_neurons if neuron[2] == 'SNN')
            
            print(f"    ğŸ“Š Top {max_retain_neurons} neurons: RNN={rnn_count_in_top}, SNN={snn_count_in_top}")
            
            # ç­–ç•¥2: è®¡ç®—é‡è¦æ€§è´¨é‡åˆ†å¸ƒ
            rnn_importance_sum = sum(neuron[1] for neuron in global_sorted if neuron[2] == 'RNN')
            snn_importance_sum = sum(neuron[1] for neuron in global_sorted if neuron[2] == 'SNN')
            total_importance = rnn_importance_sum + snn_importance_sum
            
            rnn_quality_ratio = rnn_importance_sum / total_importance
            snn_quality_ratio = snn_importance_sum / total_importance
            
            print(f"    ğŸ“ˆ Importance quality: RNN={rnn_quality_ratio:.2%}, SNN={snn_quality_ratio:.2%}")
            
            # ç­–ç•¥3: å¤šç§åˆ†é…æ–¹æ¡ˆ
            allocations = []
            
            # æ–¹æ¡ˆA: çœŸæ­£çš„å…¨å±€Top-Nåˆ†å¸ƒï¼ˆæœ€é‡è¦çš„ç­–ç•¥ï¼‰
            pure_top_rnn = sum(1 for neuron in top_neurons if neuron[2] == 'RNN')
            pure_top_snn = sum(1 for neuron in top_neurons if neuron[2] == 'SNN')
            # ç¡®ä¿æœ€å°é…é¢ä½†ä¼˜å…ˆä¿æŒåŸå§‹åˆ†å¸ƒ
            if pure_top_rnn < min_rnn_quota:
                adjustment = min_rnn_quota - pure_top_rnn
                pure_top_rnn = min_rnn_quota
                pure_top_snn = max(min_snn_quota, pure_top_snn - adjustment)
            elif pure_top_snn < min_snn_quota:
                adjustment = min_snn_quota - pure_top_snn
                pure_top_snn = min_snn_quota
                pure_top_rnn = max(min_rnn_quota, pure_top_rnn - adjustment)
            allocations.append((pure_top_rnn, pure_top_snn, "Pure Global Top-N (importance-driven)"))
            
            # æ–¹æ¡ˆB: åŸºäºTop-Nåˆ†å¸ƒä½†ç¡®ä¿æœ€å°é…é¢
            top_rnn = max(min_rnn_quota, rnn_count_in_top)
            top_snn = max(min_snn_quota, snn_count_in_top)
            if top_rnn + top_snn > max_retain_neurons:
                # æŒ‰æ¯”ä¾‹è°ƒæ•´
                excess = top_rnn + top_snn - max_retain_neurons
                if top_rnn > top_snn:
                    top_rnn = max(min_rnn_quota, top_rnn - excess)
                    top_snn = max(min_snn_quota, max_retain_neurons - top_rnn)
                else:
                    top_snn = max(min_snn_quota, top_snn - excess)
                    top_rnn = max(min_rnn_quota, max_retain_neurons - top_snn)
            allocations.append((top_rnn, top_snn, "Protected Top-N distribution"))
            
            # æ–¹æ¡ˆC: è´¨é‡åŠ æƒåˆ†é…ä½†ç¡®ä¿æœ€å°é…é¢
            quality_rnn = max(min_rnn_quota, int(max_retain_neurons * rnn_quality_ratio))
            quality_snn = max(min_snn_quota, max_retain_neurons - quality_rnn)
            if quality_rnn + quality_snn > max_retain_neurons:
                # é‡æ–°è°ƒæ•´
                remaining = max_retain_neurons - min_rnn_quota - min_snn_quota
                if remaining > 0:
                    extra_rnn = int(remaining * rnn_quality_ratio)
                    quality_rnn = min_rnn_quota + extra_rnn
                    quality_snn = min_snn_quota + (remaining - extra_rnn)
                else:
                    quality_rnn, quality_snn = min_rnn_quota, min_snn_quota
            allocations.append((quality_rnn, quality_snn, "Protected Quality-weighted"))
            
            # æ–¹æ¡ˆD: å¹³è¡¡åˆ†é…
            balanced_base_rnn = max(min_rnn_quota, int(rnn_total * 0.2))  # åŸºç¡€20%
            balanced_base_snn = max(min_snn_quota, int(snn_total * 0.2))  # åŸºç¡€20%
            
            if balanced_base_rnn + balanced_base_snn <= max_retain_neurons:
                remaining = max_retain_neurons - balanced_base_rnn - balanced_base_snn
                # å‰©ä½™æŒ‰è´¨é‡åˆ†é…
                extra_rnn = int(remaining * rnn_quality_ratio)
                extra_snn = remaining - extra_rnn
                balanced_rnn = balanced_base_rnn + extra_rnn
                balanced_snn = balanced_base_snn + extra_snn
                allocations.append((balanced_rnn, balanced_snn, "Protected Balanced allocation"))
            
            # æ–¹æ¡ˆE: ä¿å®ˆå¹³å‡åˆ†é…ï¼ˆæƒé‡æœ€ä½ï¼‰
            safe_rnn = max(min_rnn_quota, max_retain_neurons // 2)
            safe_snn = max(min_snn_quota, max_retain_neurons - safe_rnn)
            allocations.append((safe_rnn, safe_snn, "Conservative equal split"))
            
            # ç­–ç•¥4: è¯„ä¼°å„ç§åˆ†é…æ–¹æ¡ˆ
            best_allocation = None
            best_score = -1
            
            print(f"    ğŸ” Evaluating allocation strategies:")
            for rnn_budget, snn_budget, method_name in allocations:
                # ç¡®ä¿é¢„ç®—æœ‰æ•ˆ
                rnn_budget = max(min_rnn_quota, min(rnn_budget, rnn_total))
                snn_budget = max(min_snn_quota, min(snn_budget, snn_total))
                
                if rnn_budget + snn_budget > max_retain_neurons:
                    # æœ€ç»ˆè°ƒæ•´åˆ°é¢„ç®—èŒƒå›´å†…
                    excess = rnn_budget + snn_budget - max_retain_neurons
                    if rnn_budget > min_rnn_quota and excess > 0:
                        reduce_rnn = min(excess, rnn_budget - min_rnn_quota)
                        rnn_budget -= reduce_rnn
                        excess -= reduce_rnn
                    if snn_budget > min_snn_quota and excess > 0:
                        reduce_snn = min(excess, snn_budget - min_snn_quota)
                        snn_budget -= reduce_snn
                
                # è¯„ä¼°è¿™ç§åˆ†é…çš„è´¨é‡
                score = self._evaluate_allocation_quality_improved(
                    global_sorted, rnn_budget, snn_budget, rnn_total, snn_total, use_normalization
                )
                
                # è®¡ç®—è¯„åˆ†åˆ†è§£ä»¥ä¾¿è°ƒè¯•
                selected_rnn = [n for n in global_sorted if n[2] == 'RNN'][:rnn_budget]
                selected_snn = [n for n in global_sorted if n[2] == 'SNN'][:snn_budget]
                
                if use_normalization:
                    total_selected_importance = sum(n[2] for n in selected_rnn + selected_snn)
                    total_importance = sum(n[2] for n in global_sorted)
                else:
                    total_selected_importance = sum(n[1] for n in selected_rnn + selected_snn)
                    total_importance = sum(n[1] for n in global_sorted)
                
                quality_ratio = total_selected_importance / total_importance if total_importance > 0 else 0
                rnn_retention_rate = rnn_budget / rnn_total
                snn_retention_rate = snn_budget / snn_total
                diversity_bonus = 1 - abs(rnn_retention_rate - snn_retention_rate) * 0.3
                quota_bonus = 0.1 if (rnn_budget >= max(2, int(rnn_total * 0.15)) and snn_budget >= max(2, int(snn_total * 0.15))) else 0
                
                print(f"        ğŸ¯ {method_name}:")
                print(f"           Allocation: RNN={rnn_budget} ({rnn_retention_rate:.1%}), SNN={snn_budget} ({snn_retention_rate:.1%})")
                print(f"           Quality: {quality_ratio:.3f} (Ã—0.85={quality_ratio*0.85:.3f})")
                print(f"           Diversity: {diversity_bonus:.3f} (Ã—0.10={diversity_bonus*0.10:.3f})")
                print(f"           Quota: {quota_bonus:.3f} (Ã—0.05={quota_bonus*0.05:.3f})")
                print(f"           Total Score: {score:.4f}")
                
                if score > best_score:
                    best_score = score
                    best_allocation = (rnn_budget, snn_budget, method_name)
            
            if best_allocation:
                rnn_budget, snn_budget, best_method = best_allocation
                print(f"    âœ… Selected: {best_method} (Score: {best_score:.4f})")
                print(f"    ğŸ“‹ Final allocation reasoning:")
                print(f"        - This strategy achieved the highest combined score")
                print(f"        - RNN allocation: {rnn_budget}/{rnn_total} = {rnn_budget/rnn_total:.1%}")
                print(f"        - SNN allocation: {snn_budget}/{snn_total} = {snn_budget/snn_total:.1%}")
                
                # æœ€ç»ˆéªŒè¯
                print(f"    ğŸ” Final validation:")
                print(f"        RNN quota: {rnn_budget}/{rnn_total} ({rnn_budget/rnn_total:.1%}) â‰¥ {min_rnn_quota} âœ“")
                print(f"        SNN quota: {snn_budget}/{snn_total} ({snn_budget/snn_total:.1%}) â‰¥ {min_snn_quota} âœ“")
                print(f"        Total: {rnn_budget + snn_budget}/{max_retain_neurons} â‰¤ {max_retain_neurons} âœ“")
                
                return rnn_budget, snn_budget
            else:
                # æç«¯æƒ…å†µä¸‹çš„é»˜è®¤åˆ†é…
                default_rnn = max(min_rnn_quota, max_retain_neurons // 2)
                default_snn = max(min_snn_quota, max_retain_neurons - default_rnn)
                print(f"    âš ï¸  Using emergency default allocation: RNN={default_rnn}, SNN={default_snn}")
                return default_rnn, default_snn
        
        def _evaluate_allocation_quality_improved(self, global_sorted, rnn_budget, snn_budget, rnn_total, snn_total, use_normalization=False):
            """
            æ”¹è¿›çš„åˆ†é…æ–¹æ¡ˆè´¨é‡è¯„ä¼°
            """
            # è·å–å®é™…ä¼šè¢«é€‰æ‹©çš„ç¥ç»å…ƒ
            selected_rnn = [n for n in global_sorted if n[2] == 'RNN'][:rnn_budget]
            selected_snn = [n for n in global_sorted if n[2] == 'SNN'][:snn_budget]
            
            # è®¡ç®—è´¨é‡æŒ‡æ ‡ï¼ˆä½¿ç”¨é€‚å½“çš„é‡è¦æ€§å€¼ï¼‰
            if use_normalization:
                # ä½¿ç”¨å½’ä¸€åŒ–åçš„æ¯”è¾ƒå€¼
                total_selected_importance = sum(n[2] for n in selected_rnn + selected_snn)
                total_importance = sum(n[2] for n in global_sorted)
            else:
                # ä½¿ç”¨åŸå§‹é‡è¦æ€§å€¼
                total_selected_importance = sum(n[1] for n in selected_rnn + selected_snn)
                total_importance = sum(n[1] for n in global_sorted)
            
            quality_ratio = total_selected_importance / total_importance if total_importance > 0 else 0
            
            # è®¡ç®—å¤šæ ·æ€§å¥–åŠ±ï¼ˆé¿å…è¿‡åº¦åå‘æŸä¸€ç±»å‹ï¼‰
            rnn_retention_rate = rnn_budget / rnn_total
            snn_retention_rate = snn_budget / snn_total
            
            # å¤šæ ·æ€§å¥–åŠ±ï¼šä¸¤ç§ç±»å‹ä¿ç•™ç‡è¶Šæ¥è¿‘ï¼Œå¥–åŠ±è¶Šé«˜ï¼Œä½†æƒé‡é™ä½
            diversity_bonus = 1 - abs(rnn_retention_rate - snn_retention_rate) * 0.3
            
            # é…é¢æ»¡è¶³å¥–åŠ±
            quota_bonus = 0
            if rnn_budget >= max(2, int(rnn_total * 0.15)) and snn_budget >= max(2, int(snn_total * 0.15)):
                quota_bonus = 0.1
            
            # ç»¼åˆè¯„åˆ†ï¼šæ›´é‡è§†è´¨é‡ï¼Œé€‚åº¦è€ƒè™‘å¤šæ ·æ€§
            score = quality_ratio * 0.85 + diversity_bonus * 0.10 + quota_bonus * 0.05
            return score

        def _compute_hessian_importance(self, dataloader, criterion, device, batch_size, bptt, ntokens, is_loader, n_v=300):
            print("is_loader", is_loader)
            ###############
            # Here, we use the fact that Conv does not have bias term
            ###############
            if self.hessian_mode == 'trace':
                # 1.åªå¯¹ç‰¹å®šå±‚ï¼ˆSNNå’ŒRNNï¼‰è®¡ç®—Hessian
                for k, v in self.model.named_parameters():
                    if k.find("all_fc") >= 0:
                        v.requires_grad = False
                    elif k.find("wh") >= 0:
                        v.requires_grad = False
                    elif k.find("decoder") >= 0:
                        v.requires_grad = False
                    elif k.find("fv") >= 0:
                        v.requires_grad = False
                    elif k.find("encoder") >= 0:
                        v.requires_grad = False
                    else:
                        print(k, v.requires_grad)
                # 2.åŠ è½½æˆ–è€…è°ƒç”¨get_trace_hutå‡½æ•°è®¡ç®—Hessian
                trace_dir = self.trace_file_name
                print(trace_dir)
                if os.path.exists(trace_dir):
                    print(f"Loading trace from {trace_dir}")
                    import numpy as np
                    results = np.load(trace_dir, allow_pickle=True)
                else:
                    results = get_trace_hut(self.model, dataloader, criterion, n_v, batch_size, bptt, ntokens, loader=is_loader, channelwise=True, layerwise=False)
                    import numpy as np
                    np.save(self.trace_file_name, results)

                for m in self.model.parameters():
                    m.requires_grad = True

                #3.é€šé“é‡è¦æ€§å¹³å‡å€¼è®¡ç®—
                channel_trace, weighted_trace = [], []
                # resultsç»“æ„ï¼š[å±‚][é€šé“][é‡‡æ ·æ¬¡æ•°]
                #å¤„ç†å±‚ layer
                for k, layer in enumerate(results):
                    # print(k, layer)
                    channel_trace.append(torch.zeros(len(layer)))
                    weighted_trace.append(torch.zeros(len(layer)))
                    #å¤„ç†é€šé“ channel
                    for cnt, channel in enumerate(layer):
                        #print(cnt, channel.shape, len(layer))
                        # è®¡ç®—æ¯ä¸ªé€šé“çš„å¹³å‡å€¼
                        channel_trace[k][cnt] = sum(channel) / len(channel)
                #for i in channel_trace:
                    # print(len(i))
                # print(len(results), self.model.parameters())

                # 4.weightåŠ æƒé‡è¦æ€§è®¡ç®—
                # é‡è¦æ€§ = Hessianè¿¹ Ã— (æƒé‡èŒƒæ•°Â²/æƒé‡å…ƒç´ æ•°é‡)
                # Hessianè¿¹åæ˜ å‚æ•°å¯¹æŸå¤±å‡½æ•°çš„æ•æ„Ÿæ€§
                # æƒé‡èŒƒæ•°åæ˜ å‚æ•°çš„å¤§å°
                # ç»“åˆä¸¤è€…å¾—åˆ°æ›´å‡†ç¡®çš„ç¥ç»å…ƒé‡è¦æ€§è¯„ä¼°
                
                # å­˜å‚¨æƒé‡æ•°æ®ç”¨äºCSVç”Ÿæˆ
                weights_for_csv = []
                
                # åœ¨å¼€å§‹å¤„ç†æ‰€æœ‰æ¨¡å—å‰ï¼Œå…ˆé‡‡é›†æ¿€æ´»å’Œæ¢¯åº¦ä¿¡æ¯
                print("ğŸ”„ å¼€å§‹é‡‡é›†æ¿€æ´»å’Œæ¢¯åº¦ä¿¡æ¯ä»¥å¢å¼ºé‡è¦æ€§è®¡ç®—...")
                activation_importance, gradient_importance = self._collect_activation_gradient_info(dataloader, criterion, batch_size, bptt, ntokens)
                
                for k, mod in enumerate(self.modules):
                    tmp = []
                    # kï¼š   å±‚ç´¢å¼•ï¼ˆ0, 1, 2, 3å¯¹åº”snn_fc1, rnn_fc1, snn_fc2, rnn_fc2ï¼‰
                    # modï¼š æ¯ä¸ªæ¨¡å—çš„ä¿¡æ¯ï¼Œæ ¼å¼ä¸º(æ¨¡å—åç§°, æ¨¡å—å¯¹è±¡)
                    m = mod[0]
                    import copy 
                    cur_weight = copy.deepcopy(mod[1].data) #mod[1]ï¼šæ¨¡å—å¯¹è±¡ï¼ˆå¦‚Linearå±‚ï¼‰ï¼Œmod[1].dataï¼šè¯¥å±‚çš„æƒé‡å¼ é‡
                    dims = len(list(cur_weight.size()))

                    # ç»´åº¦è½¬æ¢
                    # ç›®çš„ï¼šç»Ÿä¸€æƒé‡å¼ é‡çš„æ ¼å¼ï¼Œä½¿ç¬¬ä¸€ç»´åº¦å¯¹åº”é€šé“æ•°ï¼ˆä¸Hessianè¿¹çš„é€šé“å¯¹åº”ï¼‰                
                    if dims == 2:
                        # 2ç»´æƒ…å†µï¼ˆå…¨è¿æ¥å±‚ï¼‰ï¼š
                        # åŸå§‹å½¢çŠ¶ï¼š[è¾“å‡ºç¥ç»å…ƒæ•°, è¾“å…¥ç¥ç»å…ƒæ•°]
                        # è½¬æ¢åï¼š[è¾“å…¥ç¥ç»å…ƒæ•°, è¾“å‡ºç¥ç»å…ƒæ•°]
                        cur_weight = cur_weight.permute(1, 0)
                    elif dims == 3:
                        # 3ç»´æƒ…å†µï¼ˆå¯èƒ½çš„å·ç§¯å±‚æˆ–ç‰¹æ®Šç»“æ„ï¼‰ï¼š
                        # åŸå§‹å½¢çŠ¶ï¼š[è¾“å‡ºé€šé“, è¾“å…¥é€šé“, å…¶ä»–ç»´åº¦]
                        # è½¬æ¢åï¼š[å…¶ä»–ç»´åº¦, è¾“å‡ºé€šé“, è¾“å…¥é€šé“]   
                        cur_weight = cur_weight.permute(2, 0, 1)
                    
                    # ä¿å­˜æƒé‡æ•°æ®ç”¨äºCSVç”Ÿæˆ
                    weights_for_csv.append(cur_weight)
                    
                    for cnt, channel in enumerate(cur_weight):
                        # åŸæœ‰çš„é‡è¦æ€§è®¡ç®—
                        # channel_trace[k][cnt]ï¼šä¸Šé¢è®¡ç®—å¾—åˆ°çš„ï¼Œç¬¬kå±‚ç¬¬cntä¸ªé€šé“çš„å¹³å‡Hessianè¿¹
                        # .detach()ï¼šä»è®¡ç®—å›¾ä¸­åˆ†ç¦»ï¼Œé¿å…æ¢¯åº¦è®¡ç®—
                        # .norm()**2ï¼šè®¡ç®—L2èŒƒæ•°çš„å¹³æ–¹ï¼ˆæƒé‡å‘é‡çš„å¹³æ–¹å’Œï¼‰
                        # / channel.numel()ï¼šé™¤ä»¥æƒé‡å…ƒç´ æ•°é‡ï¼Œå½’ä¸€åŒ–
                        base_importance = (channel_trace[k][cnt] * channel.detach().norm()**2 / channel.numel()).cpu().item()
                        
                        # è·å–å¢å¼ºå› å­
                        activation_factor = 1.0  # é»˜è®¤å› å­
                        gradient_factor = 1.0    # é»˜è®¤å› å­
                        
                        # å°è¯•è·å–æ¿€æ´»å¢å¼ºå› å­
                        layer_name = m
                        if layer_name in activation_importance and cnt < len(activation_importance[layer_name]):
                            #activation_factor = 1.0 + activation_importance[layer_name][cnt] * 0.2  # 20%çš„æ¿€æ´»æƒé‡
                            activation_factor = activation_importance[layer_name][cnt] * 0.3
                        #ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼csvä¸­æ˜¯æŒ‰ç…§0.1å–çš„å€¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼
                        # å°è¯•è·å–æ¢¯åº¦å¢å¼ºå› å­  
                        if layer_name in gradient_importance and cnt < len(gradient_importance[layer_name]):
                            gradient_factor = 1.0 + gradient_importance[layer_name][cnt] * 0.15  # 15%çš„æ¢¯åº¦æƒé‡
                        
                        # è®¡ç®—å¢å¼ºåçš„é‡è¦æ€§
                        #enhanced_importance = base_importance * activation_factor * gradient_factor
                        enhanced_importance = (base_importance + activation_factor * channel_trace[k][cnt])
                   
                        # å¦‚æœæœ‰å¢å¼ºä¿¡æ¯ï¼Œæ‰“å°è°ƒè¯•ä¿¡æ¯ï¼ˆä»…å‰å‡ ä¸ªç¥ç»å…ƒï¼‰
                        if cnt < 3 and (activation_factor != 1.0 or gradient_factor != 1.0):
                            print(f"  Layer {layer_name}, Neuron {cnt}: base={base_importance:.6f}, "
                                  f"act_factor={activation_factor:.3f}, grad_factor={gradient_factor:.3f}, "
                                  f"enhanced={enhanced_importance:.6f}")
                        
                        tmp.append(enhanced_importance)
                    print(m, len(tmp))
                    self.importances[str(m)] = (tmp, len(tmp))
                
                #ç”ŸæˆåŒ…å«è¯¦ç»†é‡è¦æ€§ä¿¡æ¯çš„CSVæ–‡ä»¶
                # print(f"\nğŸ“Š ç”Ÿæˆç¥ç»å…ƒé‡è¦æ€§è¯¦ç»†ä¿¡æ¯...")
                # csv_filename = f"neuron_importance_details_{self.trace_file_name.split('/')[-1].replace('.npy', '')}.csv"
                # self._save_importance_details_to_csv(channel_trace, weights_for_csv, activation_importance, gradient_importance, csv_filename)
                
                # æ³¨é‡Šæ‰å¼ºåˆ¶é€€å‡ºï¼Œè®©ç¨‹åºç»§ç»­æ‰§è¡Œå‰ªææµç¨‹
                # print(f"\nğŸ›‘ CSVæ–‡ä»¶ç”Ÿæˆå®Œæˆï¼Œç¨‹åºé€€å‡ºä»¥ä¾¿è°ƒè¯•")
                # import sys
                # sys.exit(0)

            else:
                print("Unsupported mode")
                assert False

            tmp_imp_list = list(self.importances.items())

            # 5.æ··åˆç½‘ç»œç»“æ„å¤„ç†
            rnn_list = [None, None]     # rnn_fc1, rnn_fc2
            snn_list = [None, None]     # snn_fc1, snn_fc2

            for unit in tmp_imp_list:
                if unit[0].find("rnn") >= 0 or unit[0].find("lstm") >= 0:
                    if unit[0].find("1") >= 0:
                        rnn_list[0] = unit[1][0]        # rnn_fc1å±‚çš„é‡è¦æ€§
                    else:
                        assert unit[0].find("2") >= 0
                        rnn_list[1] = unit[1][0]        # rnn_fc2å±‚çš„é‡è¦æ€§
                elif unit[0].find("snn") >= 0:
                    if unit[0].find("1") >= 0:
                        snn_list[0] = unit[1][0]        # snn_fc1å±‚çš„é‡è¦æ€§
                    else:
                        assert unit[0].find("2") >= 0
                        snn_list[1] = unit[1][0]        # snn_fc2å±‚çš„é‡è¦æ€§
                else:
                    continue

            rnn_shape = [len(rnn_list[0]), len(rnn_list[1])]
            snn_shape = [len(snn_list[0]), len(snn_list[1])]

            # 6.é‡è¦æ€§æ’åº
            rnn_tuple_list = []
            snn_tuple_list = []
            # åˆ›å»º(ç´¢å¼•, é‡è¦æ€§)å…ƒç»„åˆ—è¡¨
            for no in range(len(rnn_list[0])):
                rnn_tuple_list.append((no, rnn_list[0][no]))
            for no in range(len(rnn_list[1])):
                rnn_tuple_list.append((no + rnn_shape[0], rnn_list[1][no]))
            # æŒ‰é‡è¦æ€§é™åºæ’åº
            for no in range(len(snn_list[0])):
                snn_tuple_list.append((no, snn_list[0][no]))
            for no in range(len(snn_list[1])):
                snn_tuple_list.append((no + snn_shape[0], snn_list[1][no]))

            sorted_rnn_list = sorted(rnn_tuple_list, key=lambda x:x[1])     #, reverse=True) #æŒ‰é‡è¦æ€§é™åºæ’åº
            sorted_snn_list = sorted(snn_tuple_list, key=lambda x:x[1])     #, reverse=True) #æŒ‰é‡è¦æ€§é™åºæ’åº

            sorted_rnn_list.reverse()   #é™åºæ’åºï¼Œå¦‚[(3, 0.9)ï¼Œ(0, 0.8)ï¼Œ(4, 0.6)...]
            sorted_snn_list.reverse()   #é™åºæ’åº
            
            del rnn_list, snn_list, rnn_tuple_list, snn_tuple_list

            # 7. æ ¹æ®flagé€‰æ‹©ä¸åŒçš„å‰ªæç®—æ³•
            flag = 2
            if flag == 1:
                print(f"\n{'='*80}")
                print(f"ğŸ”¥ æ‰§è¡Œç®—æ³•1: åŸºäºé‡è¦æ€§åˆ†å¸ƒåˆ†æçš„å…¨å±€å¸•ç´¯æ‰˜æ–¹æ³•")
                print(f"{'='*80}")
                
                # 7.1 å…¨å±€å‰ªæä¼˜åŒ–ï¼šç¡®ä¿æ•´ä½“å‰ªæç‡è‡³å°‘50%
                target_retention = 0
                analysis_method = 'pareto_optimal'
                
                total_neurons = len(sorted_rnn_list) + len(sorted_snn_list)
                max_retain_neurons = int(total_neurons * 0.5)  # æœ€å¤šä¿ç•™50%
                
                print(f"\nğŸŒ Global Pruning Constraint:")
                print(f"   Total neurons: {total_neurons} (RNN: {len(sorted_rnn_list)}, SNN: {len(sorted_snn_list)})")
                print(f"   Maximum retain: {max_retain_neurons} (â‰¤50% pruning target)")
                
                # 7.2 æ™ºèƒ½é¢„ç®—åˆ†é…ç­–ç•¥
                if analysis_method == 'pareto_optimal':
                    # æ£€æŸ¥RNNå’ŒSNNé‡è¦æ€§çš„æ•°å€¼å°ºåº¦å·®å¼‚
                    rnn_importances = [imp for idx, imp in sorted_rnn_list]
                    snn_importances = [imp for idx, imp in sorted_snn_list]
                    
                    rnn_mean = np.mean(rnn_importances)
                    snn_mean = np.mean(snn_importances)
                    rnn_max = np.max(rnn_importances)
                    snn_max = np.max(snn_importances)
                    
                    print(f"\nğŸ” Scale Analysis:")
                    print(f"   RNN importance - Mean: {rnn_mean:.6f}, Max: {rnn_max:.6f}")
                    print(f"   SNN importance - Mean: {snn_mean:.6f}, Max: {snn_max:.6f}")
                    
                    # è®¡ç®—å°ºåº¦å·®å¼‚
                    scale_ratio = rnn_mean / snn_mean if snn_mean > 0 else float('inf')
                    print(f"   Scale ratio (RNN/SNN): {scale_ratio:.2f}")
                    
                    if scale_ratio > 10 or scale_ratio < 0.1:
                        print(f"   âš ï¸  Significant scale difference detected! Using normalized comparison.")
                        use_normalization = True
                    else:
                        print(f"   âœ… Scales are comparable, using direct comparison.")
                        use_normalization = False
                    
                    # åˆå¹¶æ‰€æœ‰ç¥ç»å…ƒå¹¶è¿›è¡Œå°ºåº¦å¤„ç†
                    global_neuron_list = []
                    
                    if use_normalization:
                        # æ–¹æ³•1: å½’ä¸€åŒ–å¤„ç† - å°†æ¯ç§ç±»å‹çš„é‡è¦æ€§å½’ä¸€åŒ–åˆ°[0,1]
                        rnn_min, rnn_range = np.min(rnn_importances), np.max(rnn_importances) - np.min(rnn_importances)
                        snn_min, snn_range = np.min(snn_importances), np.max(snn_importances) - np.min(snn_importances)
                        
                        # æ ‡è®°æ¥æºå¹¶æ·»åŠ å½’ä¸€åŒ–é‡è¦æ€§
                        for idx, importance in sorted_rnn_list:
                            normalized_imp = (importance - rnn_min) / rnn_range if rnn_range > 0 else 0
                            global_neuron_list.append((idx, importance, normalized_imp, 'RNN', len(sorted_rnn_list)))
                        
                        for idx, importance in sorted_snn_list:
                            normalized_imp = (importance - snn_min) / snn_range if snn_range > 0 else 0
                            global_neuron_list.append((idx, importance, normalized_imp, 'SNN', len(sorted_snn_list)))
                        
                        # æŒ‰å½’ä¸€åŒ–é‡è¦æ€§æ’åº
                        global_sorted = sorted(global_neuron_list, key=lambda x: x[2], reverse=True)
                        print(f"   ğŸ“Š Using normalized importance for global ranking")
                    else:
                        # æ–¹æ³•2: ç›´æ¥ä½¿ç”¨åŸå§‹é‡è¦æ€§
                        for idx, importance in sorted_rnn_list:
                            global_neuron_list.append((idx, importance, importance, 'RNN', len(sorted_rnn_list)))
                        
                        for idx, importance in sorted_snn_list:
                            global_neuron_list.append((idx, importance, importance, 'SNN', len(sorted_snn_list)))
                        
                        # æŒ‰åŸå§‹é‡è¦æ€§æ’åº
                        global_sorted = sorted(global_neuron_list, key=lambda x: x[2], reverse=True)
                    
                    print(f"\nğŸ”— Global Pareto Analysis:")
                    print(f"   Analyzing {len(global_sorted)} neurons globally...")
                    
                    # ä½¿ç”¨æ”¹è¿›çš„å…¨å±€å¸•ç´¯æ‰˜åˆ†æ
                    rnn_budget, snn_budget = self._global_pareto_allocation_improved(
                        global_sorted, max_retain_neurons, len(sorted_rnn_list), len(sorted_snn_list), use_normalization
                    )
                    
                    print(f"\nğŸ’° Budget Allocation Results:")
                    print(f"   RNN budget: {rnn_budget}/{len(sorted_rnn_list)} ({rnn_budget/len(sorted_rnn_list):.1%})")
                    print(f"   SNN budget: {snn_budget}/{len(sorted_snn_list)} ({snn_budget/len(sorted_snn_list):.1%})")
                    print(f"   Total retain: {rnn_budget + snn_budget}/{total_neurons} ({(rnn_budget + snn_budget)/total_neurons:.1%})")
                    
                    # 7.3 åŸºäºé¢„ç®—è¿›è¡Œå±€éƒ¨å¸•ç´¯æ‰˜ä¼˜åŒ–
                    rnn_target_retention = rnn_budget / len(sorted_rnn_list)
                    snn_target_retention = snn_budget / len(sorted_snn_list)
                else:
                    # éå¸•ç´¯æ‰˜æ–¹æ³•ä¿æŒåŸæœ‰é€»è¾‘
                    rnn_target_retention = 0.4
                    snn_target_retention = 0.2
                
                # 7.4 åˆ†æRNNé‡è¦æ€§åˆ†å¸ƒå¹¶è®¾å®šé˜ˆå€¼
                rnn_threshold, rnn_analysis = self._analyze_importance_distribution(
                    sorted_rnn_list, analysis_method, 'RNN', rnn_target_retention
                )
                
                # 7.5 åˆ†æSNNé‡è¦æ€§åˆ†å¸ƒå¹¶è®¾å®šé˜ˆå€¼
                snn_threshold, snn_analysis = self._analyze_importance_distribution(
                    sorted_snn_list, analysis_method, 'SNN', snn_target_retention
                )
                
                # 7.6 åŸºäºé˜ˆå€¼é€‰æ‹©è¦ä¿ç•™çš„ç¥ç»å…ƒ
                eff_rnns_list = []
                eff_snns_list = []
                
                # é€‰æ‹©é‡è¦æ€§å¤§äºç­‰äºé˜ˆå€¼çš„RNNç¥ç»å…ƒ
                for neuron_idx, importance in sorted_rnn_list:
                    if importance >= rnn_threshold:
                        eff_rnns_list.append(neuron_idx)
                
                # é€‰æ‹©é‡è¦æ€§å¤§äºç­‰äºé˜ˆå€¼çš„SNNç¥ç»å…ƒ
                for neuron_idx, importance in sorted_snn_list:
                    if importance >= snn_threshold:
                        eff_snns_list.append(neuron_idx)
                
                eff_rnns_number = len(eff_rnns_list)
                eff_snns_number = len(eff_snns_list)
                
                print(f"\nThreshold-based selection results:")
                print(f"  RNN neurons selected: {eff_rnns_number}/{len(sorted_rnn_list)} ({eff_rnns_number/len(sorted_rnn_list):.1%})")
                print(f"  SNN neurons selected: {eff_snns_number}/{len(sorted_snn_list)} ({eff_snns_number/len(sorted_snn_list):.1%})")

                # 7.7 å¼ºåˆ¶æ‰§è¡Œå…¨å±€50%å‰ªæçº¦æŸ - è¿™æ˜¯å…³é”®æ­¥éª¤ï¼
                current_total_retained = eff_rnns_number + eff_snns_number
                
                print(f"\nğŸš¨ Enforcing Global 50% Pruning Constraint:")
                print(f"   Current retained: {current_total_retained}/{total_neurons} ({current_total_retained/total_neurons:.1%})")
                print(f"   Maximum allowed: {max_retain_neurons}/{total_neurons} (50.0%)")
                
                if current_total_retained > max_retain_neurons:
                    print(f"   âš ï¸  VIOLATION: {current_total_retained - max_retain_neurons} neurons over budget!")
                    print(f"   ğŸ”§ Applying forced truncation to meet 50% target...")
                    
                    # éœ€è¦å‰Šå‡çš„ç¥ç»å…ƒæ•°é‡
                    excess = current_total_retained - max_retain_neurons
                    
                    # ä½¿ç”¨æ”¹è¿›çš„é¢„ç®—åˆ†é…ç»“æœè¿›è¡Œå¼ºåˆ¶æˆªæ–­
                    if analysis_method == 'pareto_optimal':
                        # ä½¿ç”¨é¢„ç®—åˆ†é…çš„ç»“æœ
                        target_rnn = rnn_budget
                        target_snn = snn_budget
                    else:
                        # æŒ‰æ¯”ä¾‹åˆ†é…åˆ°é¢„ç®—èŒƒå›´å†…
                        rnn_ratio = eff_rnns_number / current_total_retained
                        target_rnn = int(max_retain_neurons * rnn_ratio)
                        target_snn = max_retain_neurons - target_rnn
                    
                    print(f"   ğŸ“Š Target allocation: RNN={target_rnn}, SNN={target_snn}")
                    
                    # å¼ºåˆ¶æˆªæ–­åˆ°ç›®æ ‡æ•°é‡
                    if eff_rnns_number > target_rnn:
                        # ä¿ç•™å‰target_rnnä¸ªæœ€é‡è¦çš„RNNç¥ç»å…ƒ
                        rnn_with_importance = [(idx, dict(sorted_rnn_list)[idx]) for idx in eff_rnns_list]
                        rnn_with_importance.sort(key=lambda x: x[1], reverse=True)
                        eff_rnns_list = [idx for idx, _ in rnn_with_importance[:target_rnn]]
                        print(f"   âœ‚ï¸  RNN truncated: {eff_rnns_number} â†’ {len(eff_rnns_list)}")
                    
                    if eff_snns_number > target_snn:
                        # ä¿ç•™å‰target_snnä¸ªæœ€é‡è¦çš„SNNç¥ç»å…ƒ
                        snn_with_importance = [(idx, dict(sorted_snn_list)[idx]) for idx in eff_snns_list]
                        snn_with_importance.sort(key=lambda x: x[1], reverse=True)
                        eff_snns_list = [idx for idx, _ in snn_with_importance[:target_snn]]
                        print(f"   âœ‚ï¸  SNN truncated: {eff_snns_number} â†’ {len(eff_snns_list)}")
                    
                    # æ›´æ–°è®¡æ•°
                    eff_rnns_number = len(eff_rnns_list)
                    eff_snns_number = len(eff_snns_list)
                    current_total_retained = eff_rnns_number + eff_snns_number
                    
                    print(f"   âœ… After truncation: RNN={eff_rnns_number}, SNN={eff_snns_number}, Total={current_total_retained}")
                
                elif current_total_retained < max_retain_neurons:
                    # å¦‚æœä¿ç•™çš„ç¥ç»å…ƒå°‘äºé¢„ç®—ï¼Œå¯ä»¥é€‚å½“å¢åŠ ï¼ˆä½†ä¿æŒåœ¨50%ä»¥å†…ï¼‰
                    available_budget = max_retain_neurons - current_total_retained
                    print(f"   ğŸ“ˆ Under budget by {available_budget} neurons, could retain more if needed")
                    
                    # å¯é€‰ï¼šæ™ºèƒ½è¡¥å……ä¸€äº›æ¥è¿‘é˜ˆå€¼çš„ç¥ç»å…ƒ
                    if available_budget > 0 and analysis_method == 'pareto_optimal':
                        print(f"   ğŸ” Considering additional high-importance neurons within budget...")
                        
                        # ä»æœªé€‰ä¸­ä½†é‡è¦æ€§è¾ƒé«˜çš„ç¥ç»å…ƒä¸­è¡¥å……
                        additional_rnn = []
                        additional_snn = []
                        
                        # è·å–æœªé€‰ä¸­çš„RNNç¥ç»å…ƒï¼ŒæŒ‰é‡è¦æ€§æ’åº
                        unselected_rnn = [(idx, imp) for idx, imp in sorted_rnn_list if idx not in eff_rnns_list]
                        for idx, imp in unselected_rnn[:available_budget//2]:
                            additional_rnn.append(idx)
                        
                        # è·å–æœªé€‰ä¸­çš„SNNç¥ç»å…ƒï¼ŒæŒ‰é‡è¦æ€§æ’åº
                        unselected_snn = [(idx, imp) for idx, imp in sorted_snn_list if idx not in eff_snns_list]
                        for idx, imp in unselected_snn[:available_budget - len(additional_rnn)]:
                            additional_snn.append(idx)
                        
                        if additional_rnn or additional_snn:
                            eff_rnns_list.extend(additional_rnn)
                            eff_snns_list.extend(additional_snn)
                            eff_rnns_number = len(eff_rnns_list)
                            eff_snns_number = len(eff_snns_list)
                            current_total_retained = eff_rnns_number + eff_snns_number
                            print(f"   ğŸ“ˆ Added {len(additional_rnn)} RNN + {len(additional_snn)} SNN neurons")
                            print(f"   ğŸ“Š New totals: RNN={eff_rnns_number}, SNN={eff_snns_number}, Total={current_total_retained}")
                
                else:
                    print(f"   âœ… Perfect match: exactly {max_retain_neurons} neurons retained")
                
                # 7.8 æœ€ç»ˆå…¨å±€çº¦æŸéªŒè¯
                assert current_total_retained <= max_retain_neurons, f"CONSTRAINT VIOLATION: {current_total_retained} > {max_retain_neurons}"
                
                final_pruning_rate_enforced = (total_neurons - current_total_retained) / total_neurons
                print(f"\nğŸ¯ Global Constraint Enforcement Results:")
                print(f"   Retained neurons: {current_total_retained}/{total_neurons} ({current_total_retained/total_neurons:.1%})")
                print(f"   Pruning rate: {final_pruning_rate_enforced:.1%}")
                
                if final_pruning_rate_enforced >= 0.5:
                    print(f"   âœ… SUCCESS: Achieved â‰¥50% pruning target!")
                else:
                    print(f"   âŒ ERROR: Still below 50% target - this should not happen!")
                
                print(f"   ğŸ“Š Breakdown: RNN={eff_rnns_number}/{len(sorted_rnn_list)} ({eff_rnns_number/len(sorted_rnn_list):.1%}), SNN={eff_snns_number}/{len(sorted_snn_list)} ({eff_snns_number/len(sorted_snn_list):.1%})")

            elif flag == 2:
                print(f"\n{'='*80}")
                print(f"ğŸ¯ æ‰§è¡Œç®—æ³•2: ç›´æ¥Top-Kæ–¹æ³•")
                print(f"{'='*80}")
                
                # è®¾å®šç›®æ ‡ä¿ç•™ç‡ - å¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´
                target_rnn_retention = 0.1 # ä¿ç•™  %çš„RNNç¥ç»å…ƒ
                target_snn_retention = 0.9  # ä¿ç•™  %çš„SNNç¥ç»å…ƒ
                
                print(f"ğŸ¯ ç›®æ ‡ä¿ç•™ç‡: RNN={target_rnn_retention:.1%}, SNN={target_snn_retention:.1%}")
                
                # 2.1 ä½¿ç”¨top_k_directæ–¹æ³•åˆ†æRNN
                rnn_threshold, rnn_analysis = self._analyze_importance_distribution(
                    sorted_rnn_list, 'top_k_direct', 'RNN', target_rnn_retention
                )
                
                # 2.2 ä½¿ç”¨top_k_directæ–¹æ³•åˆ†æSNN
                snn_threshold, snn_analysis = self._analyze_importance_distribution(
                    sorted_snn_list, 'top_k_direct', 'SNN', target_snn_retention
                )
                
                # 2.3 åŸºäºé˜ˆå€¼é€‰æ‹©è¦ä¿ç•™çš„ç¥ç»å…ƒ
                eff_rnns_list = []
                eff_snns_list = []
                
                # é€‰æ‹©é‡è¦æ€§å¤§äºç­‰äºé˜ˆå€¼çš„RNNç¥ç»å…ƒ
                for neuron_idx, importance in sorted_rnn_list:
                    if importance >= rnn_threshold:
                        eff_rnns_list.append(neuron_idx)
                
                # é€‰æ‹©é‡è¦æ€§å¤§äºç­‰äºé˜ˆå€¼çš„SNNç¥ç»å…ƒ
                for neuron_idx, importance in sorted_snn_list:
                    if importance >= snn_threshold:
                        eff_snns_list.append(neuron_idx)
                
                eff_rnns_number = len(eff_rnns_list)
                eff_snns_number = len(eff_snns_list)
                
                print(f"\nğŸ“Š Top-K Selection Results:")
                print(f"  RNN neurons selected: {eff_rnns_number}/{len(sorted_rnn_list)} ({eff_rnns_number/len(sorted_rnn_list):.1%})")
                print(f"  SNN neurons selected: {eff_snns_number}/{len(sorted_snn_list)} ({eff_snns_number/len(sorted_snn_list):.1%})")
                
                # 2.4 è®¡ç®—æ€»ä½“å‰ªæç‡
                total_neurons = len(sorted_rnn_list) + len(sorted_snn_list)
                current_total_retained = eff_rnns_number + eff_snns_number
                pruning_rate = (total_neurons - current_total_retained) / total_neurons
                
                print(f"  Total pruning rate: {pruning_rate:.1%} ({total_neurons - current_total_retained}/{total_neurons} neurons pruned)")
                
                # 2.5 æ˜¾ç¤ºè¯¦ç»†è´¨é‡ä¿¡æ¯
                rnn_quality = rnn_analysis.get('direct_quality', 0)
                snn_quality = snn_analysis.get('direct_quality', 0)
                print(f"  Quality achieved: RNN={rnn_quality:.1%}, SNN={snn_quality:.1%}")
                
            else:
                print(f"âš ï¸  ä¸æ”¯æŒçš„å‰ªæç®—æ³• flag={flag}ï¼Œä½¿ç”¨é»˜è®¤ç®—æ³•ï¼ˆç®—æ³•1ï¼‰")
                # é€’å½’è°ƒç”¨è‡ªå·±ï¼Œä½¿ç”¨flag=1
                return self._compute_hessian_importance(dataloader, criterion, device, batch_size, bptt, ntokens, is_loader, n_v, flag=1)

            # 8. ç¡®ä¿æ¯å±‚è‡³å°‘ä¿ç•™ä¸€ä¸ªç¥ç»å…ƒï¼ˆä¿æŒåŸæœ‰çš„å®‰å…¨æœºåˆ¶ï¼‰- æ‰€æœ‰ç®—æ³•å…±ç”¨
            print(f"\nğŸ›¡ï¸  æ‰§è¡Œå±‚ç»“æ„å®‰å…¨æ£€æŸ¥...")
            rnn_layer_util = [False, False] #ä½¿ç”¨å¸ƒå°”æ•°ç»„è®°å½•æ¯ä¸€å±‚æ˜¯å¦è‡³å°‘ä¿ç•™äº†ä¸€ä¸ªç¥ç»å…ƒ
            snn_layer_util = [False, False]

            # check whether at least one neuron(rnn or snn) exists in every layer
            for neuron_idx in eff_rnns_list:
                if neuron_idx >= rnn_shape[0]:
                    rnn_layer_util[1] = True
                else:
                    rnn_layer_util[0] = True
            
            for neuron_idx in eff_snns_list:
                if neuron_idx >= snn_shape[0]:
                    snn_layer_util[1] = True
                else:
                    snn_layer_util[0] = True
            
            # fix the structure - å¦‚æœæŸå±‚æ²¡æœ‰ä¿ç•™ç¥ç»å…ƒï¼Œå¼ºåˆ¶ä¿ç•™ä¸€ä¸ª
            def not_in_one_layer(idx1, idx2, thres):
                return (idx1 < thres and idx2 >= thres) or (idx2 < thres and idx1 >= thres)
            
            # å¤„ç†RNNå±‚ç»“æ„é—®é¢˜
            if rnn_layer_util[0] is False or rnn_layer_util[1] is False:
                print("Warning: Some RNN layer has no preserved neurons, fixing structure...")
                if len(eff_rnns_list) > 0:
                    last_one = eff_rnns_list[-1]
                    # ä»æœªé€‰ä¸­çš„ç¥ç»å…ƒä¸­æ‰¾ä¸€ä¸ªæ¥æ›¿æ¢ï¼Œç¡®ä¿ä¸¤å±‚éƒ½æœ‰ç¥ç»å…ƒ
                    for neuron_idx, importance in sorted_rnn_list:
                        if neuron_idx not in eff_rnns_list:
                            if not_in_one_layer(last_one, neuron_idx, rnn_shape[0]):
                                eff_rnns_list[-1] = neuron_idx
                                print(f"  Replaced RNN neuron {last_one} with {neuron_idx} to maintain layer structure")
                                break
                else:
                    # å¦‚æœæ²¡æœ‰ä¿ç•™ä»»ä½•RNNç¥ç»å…ƒï¼Œè‡³å°‘ä¿ç•™ä¸¤ä¸ªï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰
                    if len(sorted_rnn_list) >= 2:
                        # é€‰æ‹©æ¯å±‚ä¸­é‡è¦æ€§æœ€é«˜çš„ç¥ç»å…ƒ
                        layer1_best = None
                        layer2_best = None
                        for neuron_idx, importance in sorted_rnn_list:
                            if neuron_idx < rnn_shape[0] and layer1_best is None:
                                layer1_best = neuron_idx
                            elif neuron_idx >= rnn_shape[0] and layer2_best is None:
                                layer2_best = neuron_idx
                            if layer1_best is not None and layer2_best is not None:
                                break
                        eff_rnns_list = [layer1_best, layer2_best]
                        print(f"  Force preserved RNN neurons: {eff_rnns_list}")

            # å¤„ç†SNNå±‚ç»“æ„é—®é¢˜
            if snn_layer_util[0] is False or snn_layer_util[1] is False:
                print("Warning: Some SNN layer has no preserved neurons, fixing structure...")
                if len(eff_snns_list) > 0:
                    last_one = eff_snns_list[-1]
                    # ä»æœªé€‰ä¸­çš„ç¥ç»å…ƒä¸­æ‰¾ä¸€ä¸ªæ¥æ›¿æ¢ï¼Œç¡®ä¿ä¸¤å±‚éƒ½æœ‰ç¥ç»å…ƒ
                    for neuron_idx, importance in sorted_snn_list:
                        if neuron_idx not in eff_snns_list:
                            if not_in_one_layer(last_one, neuron_idx, snn_shape[0]):
                                eff_snns_list[-1] = neuron_idx
                                print(f"  Replaced SNN neuron {last_one} with {neuron_idx} to maintain layer structure")
                                break
                else:
                    # å¦‚æœæ²¡æœ‰ä¿ç•™ä»»ä½•SNNç¥ç»å…ƒï¼Œè‡³å°‘ä¿ç•™ä¸¤ä¸ªï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰
                    if len(sorted_snn_list) >= 2:
                        # é€‰æ‹©æ¯å±‚ä¸­é‡è¦æ€§æœ€é«˜çš„ç¥ç»å…ƒ
                        layer1_best = None
                        layer2_best = None
                        for neuron_idx, importance in sorted_snn_list:
                            if neuron_idx < snn_shape[0] and layer1_best is None:
                                layer1_best = neuron_idx
                            elif neuron_idx >= snn_shape[0] and layer2_best is None:
                                layer2_best = neuron_idx
                            if layer1_best is not None and layer2_best is not None:
                                break
                        eff_snns_list = [layer1_best, layer2_best]
                        print(f"  Force preserved SNN neurons: {eff_snns_list}")

            del rnn_layer_util, snn_layer_util

            # 9. æœ€ç»ˆè¾“å‡ºï¼ˆæ‰€æœ‰ç®—æ³•å…±ç”¨ï¼‰
            eff_dict = {}
            eff_dict["rnn1"] = []
            eff_dict["rnn2"] = []
            eff_dict["snn1"] = []
            eff_dict["snn2"] = []

            for item in eff_rnns_list:
                if item < rnn_shape[0]:
                    eff_dict["rnn1"].append(item)
                else:
                    eff_dict["rnn2"].append(item - rnn_shape[0])
            
            for item in eff_snns_list:
                if item < snn_shape[0]:
                    eff_dict["snn1"].append(item)
                else:
                    eff_dict["snn2"].append(item - snn_shape[0])
            
            print(f"\nâœ… å‰ªæç®—æ³• flag={flag} æ‰§è¡Œå®Œæˆ!")
            return eff_dict

        def _global_pareto_allocation_improved(self, global_sorted, max_retain_neurons, rnn_total, snn_total, use_normalization=False):
            """
            æ”¹è¿›çš„å…¨å±€å¸•ç´¯æ‰˜é¢„ç®—åˆ†é…æ–¹æ³• - è§£å†³å°ºåº¦å·®å¼‚é—®é¢˜
            Args:
                global_sorted: å…¨å±€æ’åºçš„ç¥ç»å…ƒåˆ—è¡¨ [(idx, original_importance, comparison_value, type, layer_size), ...]
                max_retain_neurons: æœ€å¤§ä¿ç•™ç¥ç»å…ƒæ•°é‡
                rnn_total: RNNç¥ç»å…ƒæ€»æ•°
                snn_total: SNNç¥ç»å…ƒæ€»æ•°
                use_normalization: æ˜¯å¦ä½¿ç”¨äº†å½’ä¸€åŒ–
            Returns:
                (rnn_budget, snn_budget): RNNå’ŒSNNçš„é¢„ç®—åˆ†é…
            """
            print(f"    ğŸ§® Computing improved global Pareto allocation...")
            
            # è®¾ç½®æœ€å°é…é¢ä¿æŠ¤ - ç¡®ä¿æ¯ç§ç±»å‹è‡³å°‘ä¿ç•™ä¸€å®šæ¯”ä¾‹
            min_rnn_quota = max(2, int(rnn_total * 0.15))  # è‡³å°‘ä¿ç•™15%çš„RNN
            min_snn_quota = max(2, int(snn_total * 0.15))  # è‡³å°‘ä¿ç•™15%çš„SNN
            
            print(f"    ğŸ›¡ï¸  Minimum quotas: RNNâ‰¥{min_rnn_quota}, SNNâ‰¥{min_snn_quota}")
            
            # æ£€æŸ¥é…é¢æ˜¯å¦å¯è¡Œ
            if min_rnn_quota + min_snn_quota > max_retain_neurons:
                print(f"    âš ï¸  Minimum quotas exceed budget, adjusting...")
                # æŒ‰æ¯”ä¾‹è°ƒæ•´æœ€å°é…é¢
                total_min = min_rnn_quota + min_snn_quota
                min_rnn_quota = max(1, int(min_rnn_quota * max_retain_neurons / total_min))
                min_snn_quota = max(1, max_retain_neurons - min_rnn_quota)
                print(f"    ğŸ”§ Adjusted quotas: RNNâ‰¥{min_rnn_quota}, SNNâ‰¥{min_snn_quota}")
            
            # ç­–ç•¥1: åˆ†æå‰Nä¸ªæœ€é‡è¦ç¥ç»å…ƒçš„ç±»å‹åˆ†å¸ƒï¼ˆè€ƒè™‘å°ºåº¦é—®é¢˜ï¼‰
            top_neurons = global_sorted[:max_retain_neurons]
            rnn_count_in_top = sum(1 for neuron in top_neurons if neuron[3] == 'RNN')
            snn_count_in_top = sum(1 for neuron in top_neurons if neuron[3] == 'SNN')
            
            print(f"    ğŸ“Š Top {max_retain_neurons} neurons: RNN={rnn_count_in_top}, SNN={snn_count_in_top}")
            print(f"    ğŸ” Raw distribution in top neurons: RNN={rnn_count_in_top/max_retain_neurons:.1%}, SNN={snn_count_in_top/max_retain_neurons:.1%}")
            
            # åˆ†æå‰100ã€200ã€300ç­‰ä¸åŒèŒƒå›´çš„åˆ†å¸ƒï¼Œçœ‹è¶‹åŠ¿
            for check_range in [100, 200, 300, 500, max_retain_neurons]:
                if check_range <= len(global_sorted):
                    check_neurons = global_sorted[:check_range]
                    check_rnn = sum(1 for n in check_neurons if n[3] == 'RNN')
                    check_snn = sum(1 for n in check_neurons if n[3] == 'SNN')
                    print(f"        Top {check_range}: RNN={check_rnn} ({check_rnn/check_range:.1%}), SNN={check_snn} ({check_snn/check_range:.1%})")
            
            # æ‰“å°å‰20ä¸ªç¥ç»å…ƒçš„è¯¦ç»†ä¿¡æ¯
            print(f"    ğŸ” Top 20 neurons detail:")
            for i, neuron in enumerate(global_sorted[:20]):
                idx, orig_imp, comp_val, ntype, total = neuron
                print(f"        {i+1:2d}. {ntype} #{idx}: orig={orig_imp:.6f}, comp={comp_val:.6f}")
            
            # ç­–ç•¥2: è®¡ç®—è°ƒæ•´åçš„é‡è¦æ€§è´¨é‡åˆ†å¸ƒ
            if use_normalization:
                # ä½¿ç”¨å½’ä¸€åŒ–åçš„æ¯”è¾ƒå€¼
                rnn_importance_sum = sum(neuron[2] for neuron in global_sorted if neuron[3] == 'RNN')
                snn_importance_sum = sum(neuron[2] for neuron in global_sorted if neuron[3] == 'SNN')
                total_importance = rnn_importance_sum + snn_importance_sum
                print(f"    ğŸ“ˆ Normalized importance quality: RNN={rnn_importance_sum:.3f}, SNN={snn_importance_sum:.3f}")
            else:
                # ä½¿ç”¨åŸå§‹é‡è¦æ€§å€¼
                rnn_importance_sum = sum(neuron[1] for neuron in global_sorted if neuron[3] == 'RNN')
                snn_importance_sum = sum(neuron[1] for neuron in global_sorted if neuron[3] == 'SNN')
                total_importance = rnn_importance_sum + snn_importance_sum
                print(f"    ğŸ“ˆ Original importance quality: RNN={rnn_importance_sum:.6f}, SNN={snn_importance_sum:.6f}")
            
            rnn_quality_ratio = rnn_importance_sum / total_importance if total_importance > 0 else 0.5
            snn_quality_ratio = snn_importance_sum / total_importance if total_importance > 0 else 0.5
            
            print(f"    ğŸ“Š Quality ratios: RNN={rnn_quality_ratio:.2%}, SNN={snn_quality_ratio:.2%}")
            print(f"    âš–ï¸  Quality difference: {abs(rnn_quality_ratio - snn_quality_ratio):.1%} ({'Balanced' if abs(rnn_quality_ratio - snn_quality_ratio) < 0.1 else 'Imbalanced'})")
            
            # ç­–ç•¥3: å¤šç§åˆ†é…æ–¹æ¡ˆ
            allocations = []
            
            # æ–¹æ¡ˆA: çœŸæ­£çš„å…¨å±€Top-Nåˆ†å¸ƒï¼ˆæœ€é‡è¦çš„ç­–ç•¥ï¼‰
            pure_top_rnn = sum(1 for neuron in top_neurons if neuron[3] == 'RNN')
            pure_top_snn = sum(1 for neuron in top_neurons if neuron[3] == 'SNN')
            print(f"    ğŸ¯ Pure Global Analysis: RNN={pure_top_rnn}, SNN={pure_top_snn} (before quota adjustment)")
            
            # ç¡®ä¿æœ€å°é…é¢ä½†ä¼˜å…ˆä¿æŒåŸå§‹åˆ†å¸ƒ
            if pure_top_rnn < min_rnn_quota:
                adjustment = min_rnn_quota - pure_top_rnn
                pure_top_rnn = min_rnn_quota
                pure_top_snn = max(min_snn_quota, pure_top_snn - adjustment)
                print(f"    ğŸ”§ RNN quota adjustment: +{adjustment} (RNN={pure_top_rnn}, SNN={pure_top_snn})")
            elif pure_top_snn < min_snn_quota:
                adjustment = min_snn_quota - pure_top_snn
                pure_top_snn = min_snn_quota
                pure_top_rnn = max(min_rnn_quota, pure_top_rnn - adjustment)
                print(f"    ğŸ”§ SNN quota adjustment: +{adjustment} (RNN={pure_top_rnn}, SNN={pure_top_snn})")
            allocations.append((pure_top_rnn, pure_top_snn, "Pure Global Top-N (importance-driven)"))
            
            # æ–¹æ¡ˆB: åŸºäºTop-Nåˆ†å¸ƒä½†ç¡®ä¿æœ€å°é…é¢
            top_rnn = max(min_rnn_quota, rnn_count_in_top)
            top_snn = max(min_snn_quota, snn_count_in_top)
            if top_rnn + top_snn > max_retain_neurons:
                # æŒ‰æ¯”ä¾‹è°ƒæ•´
                excess = top_rnn + top_snn - max_retain_neurons
                if top_rnn > top_snn:
                    top_rnn = max(min_rnn_quota, top_rnn - excess)
                    top_snn = max(min_snn_quota, max_retain_neurons - top_rnn)
                else:
                    top_snn = max(min_snn_quota, top_snn - excess)
                    top_rnn = max(min_rnn_quota, max_retain_neurons - top_snn)
            allocations.append((top_rnn, top_snn, "Protected Top-N distribution"))
            
            # æ–¹æ¡ˆC: è´¨é‡åŠ æƒåˆ†é…ä½†ç¡®ä¿æœ€å°é…é¢
            quality_rnn = max(min_rnn_quota, int(max_retain_neurons * rnn_quality_ratio))
            quality_snn = max(min_snn_quota, max_retain_neurons - quality_rnn)
            if quality_rnn + quality_snn > max_retain_neurons:
                # é‡æ–°è°ƒæ•´
                remaining = max_retain_neurons - min_rnn_quota - min_snn_quota
                if remaining > 0:
                    extra_rnn = int(remaining * rnn_quality_ratio)
                    quality_rnn = min_rnn_quota + extra_rnn
                    quality_snn = min_snn_quota + (remaining - extra_rnn)
                else:
                    quality_rnn, quality_snn = min_rnn_quota, min_snn_quota
            allocations.append((quality_rnn, quality_snn, "Protected Quality-weighted"))
            
            # æ–¹æ¡ˆD: å¹³è¡¡åˆ†é…
            balanced_base_rnn = max(min_rnn_quota, int(rnn_total * 0.2))  # åŸºç¡€20%
            balanced_base_snn = max(min_snn_quota, int(snn_total * 0.2))  # åŸºç¡€20%
            
            if balanced_base_rnn + balanced_base_snn <= max_retain_neurons:
                remaining = max_retain_neurons - balanced_base_rnn - balanced_base_snn
                # å‰©ä½™æŒ‰è´¨é‡åˆ†é…
                extra_rnn = int(remaining * rnn_quality_ratio)
                extra_snn = remaining - extra_rnn
                balanced_rnn = balanced_base_rnn + extra_rnn
                balanced_snn = balanced_base_snn + extra_snn
                allocations.append((balanced_rnn, balanced_snn, "Protected Balanced allocation"))
            
            # æ–¹æ¡ˆE: ä¿å®ˆå¹³å‡åˆ†é…ï¼ˆæƒé‡æœ€ä½ï¼‰
            safe_rnn = max(min_rnn_quota, max_retain_neurons // 2)
            safe_snn = max(min_snn_quota, max_retain_neurons - safe_rnn)
            allocations.append((safe_rnn, safe_snn, "Conservative equal split"))
            
            # ç­–ç•¥4: è¯„ä¼°å„ç§åˆ†é…æ–¹æ¡ˆ
            best_allocation = None
            best_score = -1
            
            print(f"    ğŸ” Evaluating allocation strategies:")
            for rnn_budget, snn_budget, method_name in allocations:
                # ç¡®ä¿é¢„ç®—æœ‰æ•ˆ
                rnn_budget = max(min_rnn_quota, min(rnn_budget, rnn_total))
                snn_budget = max(min_snn_quota, min(snn_budget, snn_total))
                
                if rnn_budget + snn_budget > max_retain_neurons:
                    # æœ€ç»ˆè°ƒæ•´åˆ°é¢„ç®—èŒƒå›´å†…
                    excess = rnn_budget + snn_budget - max_retain_neurons
                    if rnn_budget > min_rnn_quota and excess > 0:
                        reduce_rnn = min(excess, rnn_budget - min_rnn_quota)
                        rnn_budget -= reduce_rnn
                        excess -= reduce_rnn
                    if snn_budget > min_snn_quota and excess > 0:
                        reduce_snn = min(excess, snn_budget - min_snn_quota)
                        snn_budget -= reduce_snn
                
                # ğŸ”§ å®Œå…¨é‡å†™è´¨é‡è¯„ä¼°é€»è¾‘
                # åŸºäºå…¨å±€é‡è¦æ€§æ’åºï¼Œé€‰æ‹©æ€»å…±(rnn_budget + snn_budget)ä¸ªæœ€é‡è¦çš„ç¥ç»å…ƒ
                total_budget = rnn_budget + snn_budget
                actual_selected_neurons = global_sorted[:total_budget]
                
                # ç»Ÿè®¡å®é™…é€‰ä¸­çš„RNNå’ŒSNNæ•°é‡
                actual_selected_rnn = sum(1 for n in actual_selected_neurons if n[3] == 'RNN')
                actual_selected_snn = sum(1 for n in actual_selected_neurons if n[3] == 'SNN')
                
                # è®¡ç®—è¿™ç§å®é™…é€‰æ‹©ä¸ç›®æ ‡åˆ†é…çš„å·®å¼‚æƒ©ç½š
                rnn_diff = abs(actual_selected_rnn - rnn_budget)
                snn_diff = abs(actual_selected_snn - snn_budget)
                allocation_penalty = (rnn_diff + snn_diff) / total_budget * 0.1  # 10%çš„æƒ©ç½šç³»æ•°
                
                # è®¡ç®—çœŸå®çš„è´¨é‡æ¯”ç‡ï¼ˆåŸºäºå®é™…ä¼šè¢«é€‰æ‹©çš„æœ€é‡è¦ç¥ç»å…ƒï¼‰
                if use_normalization:
                    actual_selected_importance = sum(n[2] for n in actual_selected_neurons)
                    total_importance = sum(n[2] for n in global_sorted)
                else:
                    actual_selected_importance = sum(n[1] for n in actual_selected_neurons)
                    total_importance = sum(n[1] for n in global_sorted)
                
                true_quality_ratio = actual_selected_importance / total_importance if total_importance > 0 else 0
                
                # è°ƒæ•´åçš„è´¨é‡è¯„åˆ† = çœŸå®è´¨é‡ - åˆ†é…åå·®æƒ©ç½š
                adjusted_quality = true_quality_ratio - allocation_penalty
                
                rnn_retention_rate = rnn_budget / rnn_total
                snn_retention_rate = snn_budget / snn_total
                diversity_bonus = 1 - abs(rnn_retention_rate - snn_retention_rate) * 0.3
                quota_bonus = 0.1 if (rnn_budget >= max(2, int(rnn_total * 0.15)) and snn_budget >= max(2, int(snn_total * 0.15))) else 0
                
                print(f"        ğŸ¯ {method_name}:")
                print(f"           Allocation: RNN={rnn_budget} ({rnn_retention_rate:.1%}), SNN={snn_budget} ({snn_retention_rate:.1%})")
                print(f"           Actual selection: RNN={actual_selected_rnn}, SNN={actual_selected_snn}")
                print(f"           Allocation penalty: {allocation_penalty:.3f} (diff: RNNÂ±{rnn_diff}, SNNÂ±{snn_diff})")
                print(f"           True quality: {true_quality_ratio:.3f}")
                print(f"           Adjusted quality: {adjusted_quality:.3f} (Ã—0.85={adjusted_quality*0.85:.3f})")
                print(f"           Diversity: {diversity_bonus:.3f} (Ã—0.10={diversity_bonus*0.10:.3f})")
                print(f"           Quota: {quota_bonus:.3f} (Ã—0.05={quota_bonus*0.05:.3f})")
                
                # ä½¿ç”¨è°ƒæ•´åçš„è´¨é‡è¯„åˆ†
                corrected_score = adjusted_quality * 0.85 + diversity_bonus * 0.10 + quota_bonus * 0.05
                print(f"           Total Score: {corrected_score:.4f}")
                
                if corrected_score > best_score:
                    best_score = corrected_score
                    best_allocation = (rnn_budget, snn_budget, method_name)
            
            if best_allocation:
                rnn_budget, snn_budget, best_method = best_allocation
                print(f"    âœ… Selected: {best_method} (Score: {best_score:.4f})")
                print(f"    ğŸ“‹ Final allocation reasoning:")
                print(f"        - This strategy achieved the highest combined score")
                print(f"        - RNN allocation: {rnn_budget}/{rnn_total} = {rnn_budget/rnn_total:.1%}")
                print(f"        - SNN allocation: {snn_budget}/{snn_total} = {snn_budget/snn_total:.1%}")
                
                # æœ€ç»ˆéªŒè¯
                print(f"    ğŸ” Final validation:")
                print(f"        RNN quota: {rnn_budget}/{rnn_total} ({rnn_budget/rnn_total:.1%}) â‰¥ {min_rnn_quota} âœ“")
                print(f"        SNN quota: {snn_budget}/{snn_total} ({snn_budget/snn_total:.1%}) â‰¥ {min_snn_quota} âœ“")
                print(f"        Total: {rnn_budget + snn_budget}/{max_retain_neurons} â‰¤ {max_retain_neurons} âœ“")
                
                return rnn_budget, snn_budget
            else:
                # æç«¯æƒ…å†µä¸‹çš„é»˜è®¤åˆ†é…
                default_rnn = max(min_rnn_quota, max_retain_neurons // 2)
                default_snn = max(min_snn_quota, max_retain_neurons - default_rnn)
                print(f"    âš ï¸  Using emergency default allocation: RNN={default_rnn}, SNN={default_snn}")
                return default_rnn, default_snn

        def _save_importance_details_to_csv(self, channel_trace, weights_data, activation_importance=None, gradient_importance=None, filename="neuron_importance_details.csv"):
            """
            å°†ç¥ç»å…ƒé‡è¦æ€§çš„è¯¦ç»†ä¿¡æ¯ä¿å­˜åˆ°CSVæ–‡ä»¶
            Args:
                channel_trace: Hessianè¿¹æ•°æ®
                weights_data: æƒé‡æ•°æ®
                activation_importance: æ¿€æ´»é‡è¦æ€§å­—å…¸ï¼ˆå¯é€‰ï¼‰
                gradient_importance: æ¢¯åº¦é‡è¦æ€§å­—å…¸ï¼ˆå¯é€‰ï¼‰
                filename: è¾“å‡ºCSVæ–‡ä»¶å
            """
            import csv
            import numpy as np
            
            print(f"æ­£åœ¨ç”Ÿæˆé‡è¦æ€§è¯¦ç»†ä¿¡æ¯CSVæ–‡ä»¶: {filename}")
            
            # å‡†å¤‡CSVæ•°æ®
            csv_data = []
            csv_headers = ['layer_name', 'neuron_index', 'hessian_trace', 'weight_norm_squared', 'weight_elements_count', 
                          'nonzero_weight_ratio', 'norm_squared_per_element', 'importance_value', 
                          'activation_importance', 'gradient_importance', 'activation_factor', 'gradient_factor', 'enhanced_importance']
            
            # éå†æ¯ä¸ªæ¨¡å—
            for k, mod in enumerate(self.modules):
                layer_name = mod[0]  # å±‚åç§°
                cur_weight = weights_data[k]  # è¯¥å±‚çš„æƒé‡æ•°æ®
                
                print(f"å¤„ç†å±‚ {layer_name}ï¼Œç¥ç»å…ƒæ•°é‡: {len(cur_weight)}")
                
                # éå†è¯¥å±‚çš„æ¯ä¸ªç¥ç»å…ƒ
                for cnt, channel in enumerate(cur_weight):
                    # è®¡ç®—å„é¡¹æŒ‡æ ‡
                    hessian_trace = channel_trace[k][cnt].cpu().item() if hasattr(channel_trace[k][cnt], 'cpu') else channel_trace[k][cnt]
                    
                    # è®¡ç®—éé›¶æƒé‡æ¯”ä¾‹
                    weight_values = channel.detach().cpu().numpy().flatten() if hasattr(channel, 'cpu') else channel.numpy().flatten()
                    nonzero_count = np.count_nonzero(weight_values)
                    total_count = len(weight_values)
                    nonzero_weight_ratio = nonzero_count / total_count if total_count > 0 else 0.0
                    
                    weight_norm_squared = channel.detach().norm()**2
                    weight_elements_count = channel.numel()
                    norm_squared_per_element = weight_norm_squared / weight_elements_count
                    importance_value = hessian_trace * norm_squared_per_element
                    
                    # è½¬æ¢ä¸ºPythonæ ‡é‡
                    weight_norm_squared = weight_norm_squared.cpu().item() if hasattr(weight_norm_squared, 'cpu') else weight_norm_squared
                    norm_squared_per_element = norm_squared_per_element.cpu().item() if hasattr(norm_squared_per_element, 'cpu') else norm_squared_per_element
                    importance_value = importance_value.cpu().item() if hasattr(importance_value, 'cpu') else importance_value
                    
                    # è·å–æ¿€æ´»å’Œæ¢¯åº¦é‡è¦æ€§ä¿¡æ¯
                    act_importance = 0.0
                    grad_importance = 0.0
                    activation_factor = 1.0
                    gradient_factor = 1.0
                    enhanced_importance = importance_value
                    
                    if activation_importance and layer_name in activation_importance and cnt < len(activation_importance[layer_name]):
                        act_importance = activation_importance[layer_name][cnt]
                        activation_factor = 1.0 + act_importance * 0.2
                    
                    if gradient_importance and layer_name in gradient_importance and cnt < len(gradient_importance[layer_name]):
                        grad_importance = gradient_importance[layer_name][cnt]
                        gradient_factor = 1.0 + grad_importance * 0.15
                    
                    # è®¡ç®—å¢å¼ºåçš„é‡è¦æ€§ - ä½¿ç”¨æ–°çš„è®¡ç®—å…¬å¼
                    enhanced_importance =( importance_value + act_importance*0.1 * hessian_trace)

                    # æ·»åŠ åˆ°CSVæ•°æ®
                    csv_data.append([
                        layer_name,
                        cnt,
                        f"{hessian_trace:.8f}",
                        f"{weight_norm_squared:.8f}", 
                        weight_elements_count,
                        f"{nonzero_weight_ratio:.6f}",
                        f"{norm_squared_per_element:.8f}",
                        f"{importance_value:.8f}",
                        f"{act_importance:.8f}",
                        f"{grad_importance:.8f}",
                        f"{activation_factor:.6f}",
                        f"{gradient_factor:.6f}",
                        f"{enhanced_importance:.8f}"
                    ])
            
            # å†™å…¥CSVæ–‡ä»¶
            try:
                with open(filename, 'w', newline='', encoding='utf-8') as csvfile:
                    writer = csv.writer(csvfile)
                    writer.writerow(csv_headers)
                    writer.writerows(csv_data)
                
                print(f"âœ… CSVæ–‡ä»¶å·²æˆåŠŸç”Ÿæˆ: {filename}")
                print(f"   åŒ…å« {len(csv_data)} è¡Œæ•°æ®ï¼ˆä¸å«è¡¨å¤´ï¼‰")
                print(f"   åŒ…å«åŸå§‹é‡è¦æ€§ã€æ¿€æ´»é‡è¦æ€§ã€æ¢¯åº¦é‡è¦æ€§å’Œå¢å¼ºé‡è¦æ€§ä¿¡æ¯")
                
                # ç»Ÿè®¡æ¯å±‚çš„ç¥ç»å…ƒæ•°é‡
                layer_stats = {}
                for row in csv_data:
                    layer_name = row[0]
                    if layer_name not in layer_stats:
                        layer_stats[layer_name] = 0
                    layer_stats[layer_name] += 1
                
                print("   å„å±‚ç¥ç»å…ƒæ•°é‡ç»Ÿè®¡:")
                for layer_name, count in layer_stats.items():
                    print(f"     {layer_name}: {count} ä¸ªç¥ç»å…ƒ")
                
                # ç»Ÿè®¡æ¿€æ´»å’Œæ¢¯åº¦å¢å¼ºçš„æ•ˆæœ
                if activation_importance or gradient_importance:
                    non_zero_act = sum(1 for row in csv_data if float(row[8]) != 0.0)
                    non_zero_grad = sum(1 for row in csv_data if float(row[9]) != 0.0)
                    enhanced_count = sum(1 for row in csv_data if float(row[12]) != float(row[7]))
                    
                    print(f"   ğŸ“ˆ æ¿€æ´»å¢å¼ºä¿¡æ¯: {non_zero_act}/{len(csv_data)} ({non_zero_act/len(csv_data)*100:.1f}%)")
                    print(f"   ğŸ“ˆ æ¢¯åº¦å¢å¼ºä¿¡æ¯: {non_zero_grad}/{len(csv_data)} ({non_zero_grad/len(csv_data)*100:.1f}%)")
                    print(f"   ğŸ”„ é‡è¦æ€§è¢«å¢å¼ºçš„ç¥ç»å…ƒ: {enhanced_count}/{len(csv_data)} ({enhanced_count/len(csv_data)*100:.1f}%)")
                    
            except Exception as e:
                print(f"âŒ å†™å…¥CSVæ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {e}")

        def _collect_activation_gradient_info(self, dataloader, criterion, batch_size, bptt, ntokens):
            """
            å®æ—¶é‡‡é›†æ¿€æ´»å’Œæ¢¯åº¦ä¿¡æ¯çš„å‡½æ•° - æ¢å¤åˆ°ä¹‹å‰ç‰ˆæœ¬
            ç›´æ¥è®¿é—®æ¨¡å‹å‚æ•°å¹¶ä½¿ç”¨çœŸå®æ•°æ®
            """
            print("ğŸ”„ å¼€å§‹é‡‡é›†æ¿€æ´»å’Œæ¢¯åº¦ä¿¡æ¯ï¼ˆæ¢å¤ç‰ˆæœ¬ï¼‰...")
            
            # å­˜å‚¨æ¿€æ´»å’Œæ¢¯åº¦ä¿¡æ¯
            layer_activations = {}
            layer_gradients = {}
            tensor_hooks = []
            
            # è·å–ç›®æ ‡å‚æ•°å¹¶ä¸ºå®ƒä»¬æ³¨å†Œhooks
            target_params = {}
            for name, param in self.model.named_parameters():
                if name in ['snn1', 'rnn1', 'snn2', 'rnn2']:
                    target_params[name] = param
                    print(f"   ğŸ“Œ æ‰¾åˆ°ç›®æ ‡å‚æ•°: {name}, å½¢çŠ¶: {param.shape}")
            
            print(f"ğŸ“‹ æ‰¾åˆ° {len(target_params)} ä¸ªç›®æ ‡å‚æ•°")
            
            if len(target_params) == 0:
                print("âš ï¸ æœªæ‰¾åˆ°ç›®æ ‡å‚æ•°ï¼Œè¿”å›ç©ºç»“æœ")
                return {}, {}
            
            # é‡å†™æ¨¡å‹çš„å‰å‘ä¼ æ’­æ¥æ’å…¥è®°å½•ç‚¹
            original_forward = self.model.forward
            
            def hooked_forward(raw_input, hidden):
                # input vector embedding
                emb = self.model.encoder(raw_input)
                input = self.model.dropout(emb)
                
                # embedded inputs forward pass
                n_win, batch_size, input_size = input.size()
                
                h1_mem = h1_spike = torch.zeros(batch_size, self.model.snn_shape[0], device = self.model.device)
                h2_mem = h2_spike = torch.zeros(batch_size, self.model.snn_shape[1], device = self.model.device)
                h1_y, h2_y = hidden
                
                buf = []
                
                # è®°å½•æ¯ä¸ªæ—¶é—´æ­¥çš„æ¿€æ´»
                if self.model.union:
                    for t in range(n_win):
                        output0 = input[t]
                        

                        # SNN1å±‚è®¡ç®—å’Œè®°å½•
                        if self.model.snn_shape[0] > 0 and hasattr(self.model, 'snn1'):
                            snn1_input = output0
                            snn1_activation = snn1_input.mm(self.model.snn1)  # çŸ©é˜µä¹˜æ³•                            
                            # è®°å½•æ¿€æ´»
                            if 'snn1' not in layer_activations:
                                layer_activations['snn1'] = []
                            if len(layer_activations['snn1']) < 5:
                                layer_activations['snn1'].append(snn1_activation.detach().cpu())
                            
                            h1_mem, h1_spike = self.model.snn_update(self.model.snn1, output0, h1_mem, h1_spike)
                        
                        # RNN1å±‚è®¡ç®—å’Œè®°å½•  
                        if self.model.rnn_shape[0] > 0 and hasattr(self.model, 'rnn1'):
                            rnn1_input = torch.cat((output0, h1_y), dim=1)
                            rnn1_activation = rnn1_input.mm(self.model.rnn1)  # çŸ©é˜µä¹˜æ³•
                            
                            # è®°å½•æ¿€æ´»
                            if 'rnn1' not in layer_activations:
                                layer_activations['rnn1'] = []
                            if len(layer_activations['rnn1']) < 5:
                                layer_activations['rnn1'].append(rnn1_activation.detach().cpu())
                            
                            h1_y = self.model.rnn_union_update(self.model.rnn1, output0, h1_y)
                        
                        output1 = torch.cat((h1_spike, h1_y), dim=1)
                        
                        # SNN2å±‚è®¡ç®—å’Œè®°å½•
                        if self.model.snn_shape[1] > 0 and hasattr(self.model, 'snn2'):
                            snn2_input = output1
                            snn2_activation = snn2_input.mm(self.model.snn2)  # çŸ©é˜µä¹˜æ³•
                            
                            # è®°å½•æ¿€æ´»
                            if 'snn2' not in layer_activations:
                                layer_activations['snn2'] = []
                            if len(layer_activations['snn2']) < 5:
                                layer_activations['snn2'].append(snn2_activation.detach().cpu())
                            
                            h2_mem, h2_spike = self.model.snn_update(self.model.snn2, output1, h2_mem, h2_spike)
                        
                        # RNN2å±‚è®¡ç®—å’Œè®°å½•
                        if self.model.rnn_shape[1] > 0 and hasattr(self.model, 'rnn2'):
                            rnn2_input = torch.cat((output1, h2_y), dim=1)
                            rnn2_activation = rnn2_input.mm(self.model.rnn2)  # çŸ©é˜µä¹˜æ³•
                            
                            # è®°å½•æ¿€æ´»
                            if 'rnn2' not in layer_activations:
                                layer_activations['rnn2'] = []
                            if len(layer_activations['rnn2']) < 5:
                                layer_activations['rnn2'].append(rnn2_activation.detach().cpu())
                            
                            h2_y = self.model.rnn_union_update(self.model.rnn2, output1, h2_y)
                        
                        output2 = torch.cat((h2_spike, h2_y), dim=1)
                        buf.append(output2)
                
                stacked_output = torch.stack(buf, dim=0)
                
                # dropout and decoded
                output = self.model.dropout(stacked_output)
                decoded = self.model.decoder(output.view(output.size(0) * output.size(1), output.size(2)))
                
                return decoded.view(output.size(0), output.size(1), decoded.size(1)), (h1_y, h2_y)
            
            # ä¸´æ—¶æ›¿æ¢å‰å‘ä¼ æ’­å‡½æ•°
            self.model.forward = hooked_forward
            
            # ä¸ºå‚æ•°æ³¨å†Œæ¢¯åº¦hooks
            def make_grad_hook(param_name):
                def hook(grad):
                    if param_name not in layer_gradients:
                        layer_gradients[param_name] = []
                    if len(layer_gradients[param_name]) < 5:
                        layer_gradients[param_name].append(grad.detach().cpu())
                        print(f"   ğŸ“Š {param_name} æ¢¯åº¦å½¢çŠ¶: {grad.shape}")
                return hook
            
            # ä¸ºç›®æ ‡å‚æ•°æ³¨å†Œæ¢¯åº¦hooks
            for param_name, param in target_params.items():
                hook = param.register_hook(make_grad_hook(param_name))
                tensor_hooks.append(hook)
                print(f"   ğŸ¯ ä¸º {param_name} æ³¨å†Œæ¢¯åº¦hook")
            
            # ä½¿ç”¨çœŸå®æ•°æ®é›†è¿›è¡Œè®­ç»ƒæ¥é‡‡é›†ä¿¡æ¯
            print("ğŸƒ ä½¿ç”¨çœŸå®æ•°æ®é›†æ‰§è¡Œå‰å‘åå‘ä¼ æ’­æ¥é‡‡é›†æ¿€æ´»å’Œæ¢¯åº¦...")
            self.model.train()
            
            try:
                # å®šä¹‰get_batchå‡½æ•°ï¼ˆå¤åˆ¶è‡ªrnn-ptb.pyï¼‰
                def get_batch(source, i):
                    seq_len = min(bptt, len(source) - 1 - i)
                    data = source[i:i + seq_len]
                    target = source[i + 1:i + 1 + seq_len].view(-1)
                    return data, target
                
                # ä½¿ç”¨çœŸå®æ•°æ®é›†
                step_count = 0
                max_steps = 3  # åªæ‰§è¡Œ3æ­¥ä»¥èŠ‚çœæ—¶é—´
                
                print(f"   ğŸ“Š ä½¿ç”¨çœŸå®æ•°æ®é›†ï¼Œæ‰¹æ¬¡å¤§å°: {batch_size}, åºåˆ—é•¿åº¦: {bptt}")
                print(f"   ğŸ“Š æ•°æ®é›†æ€»é•¿åº¦: {len(dataloader)}")
                
                for i in range(0, len(dataloader) - 1, bptt):
                    if step_count >= max_steps:
                        break
                    
                    # è·å–çœŸå®æ•°æ®æ‰¹æ¬¡
                    data, targets = get_batch(dataloader, i)
                    data = data.cuda() if torch.cuda.is_available() else data
                    targets = targets.cuda() if torch.cuda.is_available() else targets
                    
                    print(f"   ğŸ“Š Step {step_count+1}: æ•°æ®å½¢çŠ¶={data.shape}, ç›®æ ‡å½¢çŠ¶={targets.shape}")
                    
                    # åˆå§‹åŒ–éšè—çŠ¶æ€
                    hidden = self.model.init_hidden(data.size(1))  # batch_sizeæ˜¯ç¬¬äºŒç»´
                    
                    # å‰å‘ä¼ æ’­ï¼ˆä½¿ç”¨æˆ‘ä»¬çš„hookedç‰ˆæœ¬ï¼‰
                    output, hidden = self.model(data, hidden)
                    
                    # è®¡ç®—æŸå¤±å¹¶åå‘ä¼ æ’­
                    loss = criterion(output.view(-1, ntokens), targets)
                    self.model.zero_grad()
                    loss.backward()
                    
                    print(f"   ğŸ“Š Step {step_count+1}: loss={loss.item():.4f}, è¾“å‡ºå½¢çŠ¶={output.shape}")
                    step_count += 1
                    
            except Exception as e:
                print(f"   âš ï¸ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
                import traceback
                traceback.print_exc()
            
            # æ¢å¤åŸå§‹å‰å‘ä¼ æ’­å‡½æ•°
            self.model.forward = original_forward
            
            # æ¸…ç†hooks
            for hook in tensor_hooks:
                hook.remove()
            
            # å¤„ç†é‡‡é›†åˆ°çš„ä¿¡æ¯
            activation_importance = {}
            gradient_importance = {}
            
            print("ğŸ“Š å¤„ç†é‡‡é›†åˆ°çš„æ¿€æ´»ä¿¡æ¯...")
            for layer_name, activations in layer_activations.items():
                if activations and len(activations) > 0:
                    try:
                        # åˆå¹¶æ‰€æœ‰batchçš„æ¿€æ´»
                        all_acts = torch.cat(activations, dim=0)
                        if len(all_acts.shape) >= 2:
                            neuron_count = all_acts.shape[-1]
                            importance_factors = []
                            
                            for i in range(neuron_count):
                                neuron_acts = all_acts[..., i].flatten()
                                # æ¿€æ´»é¢‘ç‡ + æ¿€æ´»å¼ºåº¦
                                activation_freq = (neuron_acts > 0).float().mean().item()
                                activation_magnitude = neuron_acts.abs().mean().item()
                                factor = activation_freq * 0.5 + activation_magnitude * 0.5
                                importance_factors.append(factor)
                            
                            activation_importance[layer_name] = importance_factors
                            print(f"   âœ… {layer_name}: å¤„ç†äº† {neuron_count} ä¸ªç¥ç»å…ƒ")
                    except Exception as e:
                        print(f"   âš ï¸ å¤„ç† {layer_name} æ¿€æ´»æ—¶å‡ºé”™: {e}")
            
            print("ğŸ“Š å¤„ç†é‡‡é›†åˆ°çš„æ¢¯åº¦ä¿¡æ¯...")
            for layer_name, gradients in layer_gradients.items():
                if gradients and len(gradients) > 0:
                    try:
                        # åˆå¹¶æ‰€æœ‰batchçš„æ¢¯åº¦
                        all_grads = torch.cat(gradients, dim=0)
                        if len(all_grads.shape) >= 2:
                            neuron_count = all_grads.shape[-1]
                            importance_factors = []
                            
                            for i in range(neuron_count):
                                neuron_grads = all_grads[..., i].flatten()
                                # æ¢¯åº¦å¹…å€¼ + æ¢¯åº¦ç¨³å®šæ€§
                                gradient_magnitude = neuron_grads.abs().mean().item()
                                gradient_std = neuron_grads.std().item()
                                gradient_stability = 1.0 / (gradient_std + 1e-8)
                                factor = gradient_magnitude * 0.7 + min(gradient_stability, 10.0) * 0.3
                                importance_factors.append(factor)
                            
                            gradient_importance[layer_name] = importance_factors
                            print(f"   âœ… {layer_name}: å¤„ç†äº† {neuron_count} ä¸ªç¥ç»å…ƒ")
                    except Exception as e:
                        print(f"   âš ï¸ å¤„ç† {layer_name} æ¢¯åº¦æ—¶å‡ºé”™: {e}")
            
            print(f"âœ… æ¿€æ´»å’Œæ¢¯åº¦ä¿¡æ¯é‡‡é›†å®Œæˆ!")
            print(f"   ğŸ“Š æ¿€æ´»ä¿¡æ¯: {len(activation_importance)} å±‚")
            print(f"   ğŸ“Š æ¢¯åº¦ä¿¡æ¯: {len(gradient_importance)} å±‚")
            
            return activation_importance, gradient_importance

