#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ”¹è¿›çš„ç¥ç»å…ƒé‡è¦æ€§è®¡ç®—æ–¹æ³•
å¯ä»¥ç›´æ¥é›†æˆåˆ°ç°æœ‰çš„HessianPrunerä¸­ï¼Œæä¾›æ›´ç²¾ç»†çš„é‡è¦æ€§è¯„ä¼°

ä¸»è¦æ”¹è¿›ï¼š
1. å¤šç»´åº¦æƒé‡åˆ†æ
2. æ¿€æ´»æ¨¡å¼è€ƒè™‘
3. æ¢¯åº¦ä¿¡æ¯åˆ©ç”¨
4. ç½‘ç»œç»“æ„æ„ŸçŸ¥
5. è‡ªé€‚åº”é˜ˆå€¼é€‰æ‹©
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import copy
from collections import defaultdict
import math

class ImprovedImportanceCalculator:
    """
    æ”¹è¿›çš„é‡è¦æ€§è®¡ç®—å™¨
    å¯ä»¥æ›¿æ¢HessianPrunerä¸­çš„é‡è¦æ€§è®¡ç®—éƒ¨åˆ†
    """
    
    def __init__(self, use_activation_analysis=True, use_gradient_analysis=True, 
                 use_structural_analysis=True, activation_samples=100):
        """
        åˆå§‹åŒ–æ”¹è¿›çš„é‡è¦æ€§è®¡ç®—å™¨
        
        Args:
            use_activation_analysis: æ˜¯å¦ä½¿ç”¨æ¿€æ´»åˆ†æ
            use_gradient_analysis: æ˜¯å¦ä½¿ç”¨æ¢¯åº¦åˆ†æ
            use_structural_analysis: æ˜¯å¦ä½¿ç”¨ç»“æ„åˆ†æ
            activation_samples: æ¿€æ´»åˆ†æçš„æ ·æœ¬æ•°é‡
        """
        self.use_activation_analysis = use_activation_analysis
        self.use_gradient_analysis = use_gradient_analysis
        self.use_structural_analysis = use_structural_analysis
        self.activation_samples = activation_samples
        
        # å­˜å‚¨æ¿€æ´»å’Œæ¢¯åº¦ä¿¡æ¯
        self.layer_activations = defaultdict(list)
        self.layer_gradients = defaultdict(list)
        self.hooks = []
    
    def compute_enhanced_importance(self, model, modules, channel_trace, dataloader=None, 
                                  criterion=None, device="cuda"):
        """
        è®¡ç®—å¢å¼ºçš„ç¥ç»å…ƒé‡è¦æ€§
        
        Args:
            model: è¦åˆ†æçš„æ¨¡å‹
            modules: æ¨¡å—åˆ—è¡¨ï¼ˆæ¥è‡ªHessianPruner._prepare_modelï¼‰
            channel_trace: Hessianè¿¹ä¿¡æ¯
            dataloader: æ•°æ®åŠ è½½å™¨ï¼ˆç”¨äºæ¿€æ´»å’Œæ¢¯åº¦åˆ†æï¼‰
            criterion: æŸå¤±å‡½æ•°
            device: è®¾å¤‡
            
        Returns:
            dict: å¢å¼ºçš„é‡è¦æ€§åˆ†æ•°
        """
        print("ğŸ”¬ è®¡ç®—å¢å¼ºçš„ç¥ç»å…ƒé‡è¦æ€§...")
        
        # 1. è®¡ç®—åŸºç¡€é‡è¦æ€§ï¼ˆæ”¹è¿›ç‰ˆçš„ä¼ ç»Ÿæ–¹æ³•ï¼‰
        print("   ğŸ“Š è®¡ç®—å¢å¼ºåŸºç¡€é‡è¦æ€§...")
        basic_importance = self._compute_enhanced_basic_importance(modules, channel_trace)
        
        # 2. æ”¶é›†æ¿€æ´»å’Œæ¢¯åº¦ä¿¡æ¯ï¼ˆå¦‚æœå¯ç”¨ä¸”æœ‰æ•°æ®ï¼‰
        activation_importance = {}
        gradient_importance = {}
        
        if dataloader is not None and (self.use_activation_analysis or self.use_gradient_analysis):
            print("   ğŸ“¡ æ”¶é›†è¿è¡Œæ—¶ä¿¡æ¯...")
            self._collect_runtime_info(model, dataloader, criterion, device)
            
            if self.use_activation_analysis:
                print("   ğŸ¯ åˆ†ææ¿€æ´»æ¨¡å¼...")
                activation_importance = self._compute_activation_importance()
            
            if self.use_gradient_analysis:
                print("   ğŸŒŠ åˆ†ææ¢¯åº¦æµ...")
                gradient_importance = self._compute_gradient_importance()
        
        # 3. è®¡ç®—ç»“æ„é‡è¦æ€§
        structural_importance = {}
        if self.use_structural_analysis:
            print("   ğŸ—ï¸ åˆ†æç½‘ç»œç»“æ„...")
            structural_importance = self._compute_structural_importance(modules)
        
        # 4. èåˆæ‰€æœ‰é‡è¦æ€§åˆ†æ•°
        print("   ğŸ”€ èåˆé‡è¦æ€§åˆ†æ•°...")
        fused_importance = self._fuse_importance_scores(
            basic_importance, activation_importance, 
            gradient_importance, structural_importance
        )
        
        # æ¸…ç†hooks
        self._cleanup()
        
        return fused_importance
    
    def _compute_enhanced_basic_importance(self, modules, channel_trace):
        """
        è®¡ç®—å¢å¼ºçš„åŸºç¡€é‡è¦æ€§
        æ”¹è¿›ä¼ ç»Ÿçš„ Hessian_trace Ã— weight_norm æ–¹æ³•
        """
        enhanced_importance = {}
        
        for k, mod in enumerate(modules):
            tmp = []
            m = mod[0]  # æ¨¡å—åç§°
            cur_weight = copy.deepcopy(mod[1].data)
            dims = len(list(cur_weight.size()))
            
            # ç»´åº¦è½¬æ¢ï¼ˆä¸åŸä»£ç ä¿æŒä¸€è‡´ï¼‰
            if dims == 2:
                cur_weight = cur_weight.permute(1, 0)
            elif dims == 3:
                cur_weight = cur_weight.permute(2, 0, 1)
            
            for cnt, channel in enumerate(cur_weight):
                # è·å–Hessianè¿¹
                hessian_trace = channel_trace[k][cnt]
                
                # 1. åŸå§‹é‡è¦æ€§ï¼ˆL2èŒƒæ•°ï¼‰
                l2_norm_sq = channel.detach().norm()**2
                original_importance = (hessian_trace * l2_norm_sq / channel.numel()).cpu().item()
                
                # 2. å¢å¼ºæŒ‡æ ‡
                channel_cpu = channel.detach().cpu()
                
                # L1èŒƒæ•°ï¼ˆç¨€ç–æ€§ï¼‰
                l1_norm = torch.norm(channel_cpu, p=1).item()
                
                # æƒé‡æ–¹å·®ï¼ˆå¤šæ ·æ€§ï¼‰
                weight_variance = torch.var(channel_cpu).item()
                
                # éé›¶æƒé‡æ¯”ä¾‹ï¼ˆè¿æ¥æœ‰æ•ˆæ€§ï¼‰
                nonzero_ratio = (channel_cpu.abs() > 1e-6).float().mean().item()
                
                # æƒé‡ç†µï¼ˆä¿¡æ¯å†…å®¹ï¼‰
                abs_weights = channel_cpu.abs()
                if abs_weights.sum() > 1e-8:
                    normalized_weights = abs_weights / abs_weights.sum()
                    weight_entropy = -(normalized_weights * torch.log(normalized_weights + 1e-8)).sum().item()
                else:
                    weight_entropy = 0.0
                
                # æƒé‡åŠ¨æ€èŒƒå›´
                weight_range = (channel_cpu.max() - channel_cpu.min()).item()
                
                # æƒé‡çš„æœ‰æ•ˆç»´åº¦ï¼ˆç®€åŒ–ç‰ˆPCAï¼‰
                if len(channel_cpu.shape) > 1:
                    try:
                        U, S, V = torch.svd(channel_cpu.unsqueeze(0))
                        effective_rank = (S > S.max() * 0.01).sum().item()
                    except:
                        effective_rank = 1.0
                else:
                    effective_rank = 1.0
                
                # 3. è‡ªé€‚åº”æƒé‡è®¡ç®—
                # æ ¹æ®Hessianè¿¹çš„å¤§å°è°ƒæ•´å„ä¸ªæŒ‡æ ‡çš„æƒé‡
                hessian_magnitude = abs(hessian_trace.item())
                
                if hessian_magnitude > 1e-3:  # é«˜æ•æ„Ÿæ€§ç¥ç»å…ƒ
                    # æ›´é‡è§†åŸå§‹é‡è¦æ€§å’Œæƒé‡å¤§å°
                    enhanced_importance_score = (
                        original_importance * 0.5 +        # åŸå§‹é‡è¦æ€§
                        l2_norm_sq.item() * 0.2 +          # æƒé‡å¤§å°
                        weight_variance * 0.15 +           # æƒé‡å¤šæ ·æ€§
                        nonzero_ratio * 0.1 +              # è¿æ¥æœ‰æ•ˆæ€§
                        weight_entropy * 0.05              # ä¿¡æ¯å†…å®¹
                    )
                elif hessian_magnitude > 1e-6:  # ä¸­ç­‰æ•æ„Ÿæ€§ç¥ç»å…ƒ
                    # å¹³è¡¡è€ƒè™‘å„ä¸ªå› ç´ 
                    enhanced_importance_score = (
                        original_importance * 0.3 +        # åŸå§‹é‡è¦æ€§
                        weight_variance * 0.25 +           # æƒé‡å¤šæ ·æ€§
                        nonzero_ratio * 0.2 +              # è¿æ¥æœ‰æ•ˆæ€§
                        weight_entropy * 0.15 +            # ä¿¡æ¯å†…å®¹
                        effective_rank * 0.1               # è¡¨ç¤ºèƒ½åŠ›
                    )
                else:  # ä½æ•æ„Ÿæ€§ç¥ç»å…ƒ
                    # æ›´é‡è§†æƒé‡çš„ç»Ÿè®¡ç‰¹æ€§
                    enhanced_importance_score = (
                        weight_variance * 0.35 +           # æƒé‡å¤šæ ·æ€§
                        nonzero_ratio * 0.25 +             # è¿æ¥æœ‰æ•ˆæ€§
                        weight_entropy * 0.2 +             # ä¿¡æ¯å†…å®¹
                        weight_range * 0.1 +               # åŠ¨æ€èŒƒå›´
                        original_importance * 0.1          # åŸå§‹é‡è¦æ€§
                    )
                
                tmp.append(enhanced_importance_score)
            
            # æå–å±‚åï¼ˆå¦‚snn_fc1, rnn_fc1ç­‰ï¼‰
            layer_name = m
            enhanced_importance[layer_name] = tmp
        
        return enhanced_importance
    
    def _collect_runtime_info(self, model, dataloader, criterion, device):
        """æ”¶é›†è¿è¡Œæ—¶çš„æ¿€æ´»å’Œæ¢¯åº¦ä¿¡æ¯"""
        self.layer_activations.clear()
        self.layer_gradients.clear()
        
        # æ³¨å†Œhooks
        def make_activation_hook(name):
            def hook(module, input, output):
                if len(self.layer_activations[name]) < 20:  # é™åˆ¶æ ·æœ¬æ•°é‡
                    if isinstance(output, torch.Tensor):
                        self.layer_activations[name].append(output.detach().cpu())
            return hook
        
        def make_gradient_hook(name):
            def hook(module, grad_input, grad_output):
                if len(self.layer_gradients[name]) < 20:  # é™åˆ¶æ ·æœ¬æ•°é‡
                    if grad_output[0] is not None:
                        self.layer_gradients[name].append(grad_output[0].detach().cpu())
            return hook
        
        # ä¸ºå…³é”®å±‚æ³¨å†Œhooks
        target_layers = ['snn_fc1', 'rnn_fc1', 'snn_fc2', 'rnn_fc2']
        for name, module in model.named_modules():
            if hasattr(module, 'weight') and any(layer in name for layer in target_layers):
                if self.use_activation_analysis:
                    hook = module.register_forward_hook(make_activation_hook(name))
                    self.hooks.append(hook)
                if self.use_gradient_analysis:
                    hook = module.register_backward_hook(make_gradient_hook(name))
                    self.hooks.append(hook)
        
        # æ”¶é›†æ•°æ®
        model.train() if self.use_gradient_analysis else model.eval()
        sample_count = 0
        
        for batch_idx, batch_data in enumerate(dataloader):
            if sample_count >= self.activation_samples:
                break
            
            if len(batch_data) == 2:
                data, targets = batch_data
            else:
                data, targets = batch_data[0], batch_data[1]
            
            data = data.to(device)
            targets = targets.to(device)
            
            # å‰å‘ä¼ æ’­
            hidden = model.init_hidden(data.size(1))
            output, hidden = model(data, hidden)
            
            # åå‘ä¼ æ’­ï¼ˆå¦‚æœéœ€è¦æ¢¯åº¦ï¼‰
            if self.use_gradient_analysis and criterion is not None:
                loss = criterion(output.view(-1, output.size(-1)), targets.view(-1))
                model.zero_grad()
                loss.backward()
            
            sample_count += data.size(1)
            
            if batch_idx >= 10:  # é™åˆ¶æ‰¹æ¬¡æ•°é‡
                break
    
    def _compute_activation_importance(self):
        """åŸºäºæ¿€æ´»æ¨¡å¼è®¡ç®—é‡è¦æ€§"""
        activation_importance = {}
        
        for layer_name, activations in self.layer_activations.items():
            if not activations or len(activations) == 0:
                continue
            
            # ç®€åŒ–å±‚åæ˜ å°„
            simple_name = self._simplify_layer_name(layer_name)
            if simple_name is None:
                continue
            
            try:
                # æ‹¼æ¥æ‰€æœ‰æ¿€æ´»
                all_activations = torch.cat(activations, dim=0)
                if len(all_activations.shape) < 2:
                    continue
                
                neuron_importance = []
                
                # åˆ†ææ¯ä¸ªç¥ç»å…ƒçš„æ¿€æ´»æ¨¡å¼
                for neuron_idx in range(all_activations.shape[-1]):
                    neuron_acts = all_activations[..., neuron_idx].flatten()
                    
                    # æ¿€æ´»é¢‘ç‡
                    activation_freq = (neuron_acts > 0).float().mean().item()
                    
                    # æ¿€æ´»å¼ºåº¦
                    activation_magnitude = neuron_acts.abs().mean().item()
                    
                    # æ¿€æ´»ç¨³å®šæ€§ï¼ˆè´Ÿå˜å¼‚ç³»æ•°ï¼‰
                    activation_std = neuron_acts.std().item()
                    activation_mean = neuron_acts.mean().item()
                    if abs(activation_mean) > 1e-8:
                        activation_stability = 1.0 / (abs(activation_std / activation_mean) + 1e-8)
                    else:
                        activation_stability = 1.0
                    
                    # æ¿€æ´»åŠ¨æ€èŒƒå›´
                    activation_range = neuron_acts.max().item() - neuron_acts.min().item()
                    
                    # ç»¼åˆæ¿€æ´»é‡è¦æ€§
                    importance = (
                        activation_freq * 0.2 +
                        activation_magnitude * 0.3 +
                        activation_stability * 0.2 +
                        activation_range * 0.3
                    )
                    
                    neuron_importance.append(importance)
                
                activation_importance[simple_name] = neuron_importance
                
            except Exception as e:
                print(f"     âš ï¸ æ¿€æ´»åˆ†æå¤±è´¥ {layer_name}: {e}")
                continue
        
        return activation_importance
    
    def _compute_gradient_importance(self):
        """åŸºäºæ¢¯åº¦æµè®¡ç®—é‡è¦æ€§"""
        gradient_importance = {}
        
        for layer_name, gradients in self.layer_gradients.items():
            if not gradients or len(gradients) == 0:
                continue
            
            simple_name = self._simplify_layer_name(layer_name)
            if simple_name is None:
                continue
            
            try:
                # æ‹¼æ¥æ‰€æœ‰æ¢¯åº¦
                all_gradients = torch.cat(gradients, dim=0)
                if len(all_gradients.shape) < 2:
                    continue
                
                neuron_importance = []
                
                for neuron_idx in range(all_gradients.shape[-1]):
                    neuron_grads = all_gradients[..., neuron_idx].flatten()
                    
                    # æ¢¯åº¦å¹…å€¼
                    gradient_magnitude = neuron_grads.abs().mean().item()
                    
                    # æ¢¯åº¦ä¸€è‡´æ€§
                    sign_changes = (neuron_grads[1:] * neuron_grads[:-1] < 0).float().mean().item()
                    gradient_consistency = 1.0 - sign_changes
                    
                    # æ¢¯åº¦ä¿¡å™ªæ¯”
                    signal = neuron_grads.abs().mean().item()
                    noise = neuron_grads.std().item()
                    snr = signal / (noise + 1e-8)
                    
                    # ç»¼åˆæ¢¯åº¦é‡è¦æ€§
                    importance = (
                        gradient_magnitude * 0.5 +
                        gradient_consistency * 0.3 +
                        min(snr, 10.0) * 0.2  # é™åˆ¶SNRçš„å½±å“
                    )
                    
                    neuron_importance.append(importance)
                
                gradient_importance[simple_name] = neuron_importance
                
            except Exception as e:
                print(f"     âš ï¸ æ¢¯åº¦åˆ†æå¤±è´¥ {layer_name}: {e}")
                continue
        
        return gradient_importance
    
    def _compute_structural_importance(self, modules):
        """è®¡ç®—åŸºäºç½‘ç»œç»“æ„çš„é‡è¦æ€§"""
        structural_importance = {}
        
        # å±‚çš„ä½ç½®æƒé‡
        layer_weights = {
            'snn_fc1': 0.8,  # ç¬¬ä¸€å±‚SNN
            'rnn_fc1': 0.8,  # ç¬¬ä¸€å±‚RNN
            'snn_fc2': 0.9,  # ç¬¬äºŒå±‚SNN
            'rnn_fc2': 0.9   # ç¬¬äºŒå±‚RNN
        }
        
        # ç½‘ç»œç±»å‹æƒé‡
        network_weights = {
            'snn': 0.7,  # SNNçš„ç¦»æ•£ç‰¹æ€§
            'rnn': 0.8   # RNNçš„è¿ç»­ç‰¹æ€§
        }
        
        for k, mod in enumerate(modules):
            layer_name = mod[0]
            weights = mod[1].data
            
            # ç¡®å®šå±‚çš„ä½ç½®å’Œç½‘ç»œç±»å‹
            position_weight = layer_weights.get(layer_name, 0.5)
            network_type = 'snn' if 'snn' in layer_name else 'rnn'
            network_weight = network_weights.get(network_type, 0.5)
            
            # è®¡ç®—æ¯ä¸ªç¥ç»å…ƒçš„ç»“æ„é‡è¦æ€§
            neuron_importance = []
            for neuron_idx in range(weights.shape[0]):
                # åŸºç¡€ç»“æ„é‡è¦æ€§
                base_importance = position_weight * network_weight
                
                # è¿æ¥å¯†åº¦ï¼ˆè¯¥ç¥ç»å…ƒå‚ä¸çš„è¿æ¥æ•°é‡çš„ç›¸å¯¹æ¯”ä¾‹ï¼‰
                connection_density = 1.0  # åœ¨å…¨è¿æ¥å±‚ä¸­ï¼Œæ¯ä¸ªç¥ç»å…ƒéƒ½æœ‰ç›¸åŒçš„è¿æ¥æ•°
                
                # å±‚é—´é‡è¦æ€§ï¼ˆè€ƒè™‘cascadeè¿æ¥ï¼‰
                if 'fc2' in layer_name:
                    # ç¬¬äºŒå±‚æ¥æ”¶æ¥è‡ªç¬¬ä¸€å±‚çš„mixedä¿¡æ¯ï¼Œæ›´é‡è¦
                    layer_importance = 1.2
                else:
                    layer_importance = 1.0
                
                structural_score = base_importance * connection_density * layer_importance
                neuron_importance.append(structural_score)
            
            structural_importance[layer_name] = neuron_importance
        
        return structural_importance
    
    def _simplify_layer_name(self, layer_name):
        """ç®€åŒ–å±‚åï¼Œæ˜ å°„åˆ°æ ‡å‡†æ ¼å¼"""
        if 'snn_fc1' in layer_name or 'snn.0' in layer_name:
            return 'snn_fc1'
        elif 'rnn_fc1' in layer_name or 'rnn.0' in layer_name:
            return 'rnn_fc1'
        elif 'snn_fc2' in layer_name or 'snn.1' in layer_name:
            return 'snn_fc2'
        elif 'rnn_fc2' in layer_name or 'rnn.1' in layer_name:
            return 'rnn_fc2'
        else:
            return None
    
    def _fuse_importance_scores(self, basic_importance, activation_importance, 
                               gradient_importance, structural_importance):
        """èåˆå¤šç§é‡è¦æ€§åˆ†æ•°"""
        fused_importance = {}
        
        # å®šä¹‰æƒé‡
        weights = {
            'basic': 0.4,       # åŸºç¡€é‡è¦æ€§
            'activation': 0.25, # æ¿€æ´»é‡è¦æ€§
            'gradient': 0.2,    # æ¢¯åº¦é‡è¦æ€§
            'structural': 0.15  # ç»“æ„é‡è¦æ€§
        }
        
        # è·å–æ‰€æœ‰å±‚
        all_layers = set(basic_importance.keys())
        
        for layer in all_layers:
            # è·å–åŸºç¡€é‡è¦æ€§
            basic_scores = basic_importance.get(layer, [])
            num_neurons = len(basic_scores)
            
            if num_neurons == 0:
                continue
            
            # åˆå§‹åŒ–èåˆåˆ†æ•°
            fused_scores = np.array(basic_scores) * weights['basic']
            total_weight = weights['basic']
            
            # æ·»åŠ æ¿€æ´»é‡è¦æ€§
            if layer in activation_importance:
                act_scores = activation_importance[layer]
                if len(act_scores) == num_neurons:
                    # å½’ä¸€åŒ–åˆ°ä¸åŸºç¡€é‡è¦æ€§ç›¸åŒçš„å°ºåº¦
                    act_array = np.array(act_scores)
                    if act_array.max() > act_array.min():
                        act_normalized = (act_array - act_array.min()) / (act_array.max() - act_array.min())
                        # ç¼©æ”¾åˆ°åŸºç¡€é‡è¦æ€§çš„èŒƒå›´
                        basic_array = np.array(basic_scores)
                        act_scaled = act_normalized * (basic_array.max() - basic_array.min()) + basic_array.min()
                        fused_scores += act_scaled * weights['activation']
                        total_weight += weights['activation']
            
            # æ·»åŠ æ¢¯åº¦é‡è¦æ€§
            if layer in gradient_importance:
                grad_scores = gradient_importance[layer]
                if len(grad_scores) == num_neurons:
                    grad_array = np.array(grad_scores)
                    if grad_array.max() > grad_array.min():
                        grad_normalized = (grad_array - grad_array.min()) / (grad_array.max() - grad_array.min())
                        basic_array = np.array(basic_scores)
                        grad_scaled = grad_normalized * (basic_array.max() - basic_array.min()) + basic_array.min()
                        fused_scores += grad_scaled * weights['gradient']
                        total_weight += weights['gradient']
            
            # æ·»åŠ ç»“æ„é‡è¦æ€§
            if layer in structural_importance:
                struct_scores = structural_importance[layer]
                if len(struct_scores) == num_neurons:
                    struct_array = np.array(struct_scores)
                    basic_array = np.array(basic_scores)
                    # ç»“æ„é‡è¦æ€§ä½œä¸ºä¹˜æ³•å› å­
                    fused_scores *= struct_array
                    total_weight += weights['structural']
            
            # å½’ä¸€åŒ–
            if total_weight > 0:
                fused_scores /= total_weight
            
            fused_importance[layer] = fused_scores.tolist()
        
        return fused_importance
    
    def _cleanup(self):
        """æ¸…ç†hooks"""
        for hook in self.hooks:
            hook.remove()
        self.hooks.clear()
        self.layer_activations.clear()
        self.layer_gradients.clear()

def create_adaptive_retention_strategy():
    """
    åˆ›å»ºè‡ªé€‚åº”ä¿ç•™ç­–ç•¥
    æ ¹æ®é‡è¦æ€§åˆ†å¸ƒè‡ªåŠ¨è°ƒæ•´ä¿ç•™ç‡
    """
    def adaptive_retention_rate(importance_scores, network_type='mixed'):
        """
        è‡ªé€‚åº”ç¡®å®šç¥ç»å…ƒä¿ç•™ç‡
        
        Args:
            importance_scores: é‡è¦æ€§åˆ†æ•°åˆ—è¡¨
            network_type: ç½‘ç»œç±»å‹ ('rnn', 'snn', 'mixed')
            
        Returns:
            float: å»ºè®®çš„ä¿ç•™ç‡
        """
        if not importance_scores:
            return 0.3  # é»˜è®¤ä¿ç•™ç‡
        
        scores = np.array(importance_scores)
        
        # è®¡ç®—åˆ†å¸ƒç‰¹å¾
        mean_score = scores.mean()
        std_score = scores.std()
        cv = std_score / (mean_score + 1e-8)  # å˜å¼‚ç³»æ•°
        
        # è®¡ç®—é‡è¦æ€§é›†ä¸­åº¦
        sorted_scores = np.sort(scores)[::-1]  # é™åº
        cumsum_scores = np.cumsum(sorted_scores)
        total_importance = cumsum_scores[-1]
        
        # è®¡ç®—è¾¾åˆ°ä¸åŒé‡è¦æ€§é˜ˆå€¼éœ€è¦çš„ç¥ç»å…ƒæ¯”ä¾‹
        thresholds = [0.7, 0.8, 0.9]
        threshold_ratios = []
        
        for threshold in thresholds:
            target = total_importance * threshold
            idx = np.argmax(cumsum_scores >= target)
            ratio = (idx + 1) / len(scores)
            threshold_ratios.append(ratio)
        
        # åŸºäºåˆ†å¸ƒç‰¹å¾é€‰æ‹©ä¿ç•™ç­–ç•¥
        if cv > 2.5:  # æé«˜å˜å¼‚æ€§ï¼šé‡è¦æ€§é«˜åº¦é›†ä¸­
            base_retention = threshold_ratios[0]  # 70%é‡è¦æ€§é˜ˆå€¼
        elif cv > 1.5:  # é«˜å˜å¼‚æ€§ï¼šé‡è¦æ€§æ¯”è¾ƒé›†ä¸­
            base_retention = threshold_ratios[1]  # 80%é‡è¦æ€§é˜ˆå€¼
        elif cv > 0.8:  # ä¸­ç­‰å˜å¼‚æ€§ï¼šé‡è¦æ€§åˆ†å¸ƒä¸­ç­‰
            base_retention = threshold_ratios[2]  # 90%é‡è¦æ€§é˜ˆå€¼
        else:  # ä½å˜å¼‚æ€§ï¼šé‡è¦æ€§åˆ†å¸ƒå‡åŒ€
            base_retention = 0.6  # ä¿å®ˆç­–ç•¥
        
        # ç½‘ç»œç±»å‹è°ƒæ•´
        if network_type == 'rnn':
            # RNNæ›´ç¨³å®šï¼Œå¯ä»¥æ›´æ¿€è¿›åœ°å‰ªæ
            adjusted_retention = base_retention * 0.85
        elif network_type == 'snn':
            # SNNçš„ç¦»æ•£ç‰¹æ€§æ›´æ•æ„Ÿï¼Œä¿å®ˆä¸€äº›
            adjusted_retention = base_retention * 1.15
        else:  # mixed
            adjusted_retention = base_retention
        
        # ç¡®ä¿åœ¨åˆç†èŒƒå›´å†…
        final_retention = np.clip(adjusted_retention, 0.05, 0.8)
        
        return final_retention
    
    return adaptive_retention_rate

# ä½¿ç”¨ç¤ºä¾‹
def integrate_with_hessian_pruner():
    """
    å±•ç¤ºå¦‚ä½•å°†æ”¹è¿›çš„é‡è¦æ€§è®¡ç®—é›†æˆåˆ°ç°æœ‰çš„HessianPrunerä¸­
    """
    print("ğŸ”§ é›†æˆæ”¹è¿›çš„é‡è¦æ€§è®¡ç®—åˆ°HessianPruner...")
    
    # åœ¨HessianPrunerçš„_compute_hessian_importanceæ–¹æ³•ä¸­ï¼Œ
    # å°†åŸæ¥çš„é‡è¦æ€§è®¡ç®—éƒ¨åˆ†æ›¿æ¢ä¸ºï¼š
    
    example_code = '''
    # åœ¨HessianPruner._compute_hessian_importanceæ–¹æ³•ä¸­ï¼š
    
    # åŸæ¥çš„ä»£ç ï¼š
    # for k, mod in enumerate(self.modules):
    #     tmp = []
    #     for cnt, channel in enumerate(cur_weight):
    #         tmp.append((channel_trace[k][cnt] * channel.detach().norm()**2 / channel.numel()).cpu().item())
    #     self.importances[str(m)] = (tmp, len(tmp))
    
    # æ›¿æ¢ä¸ºï¼š
    improved_calculator = ImprovedImportanceCalculator(
        use_activation_analysis=True,
        use_gradient_analysis=True,
        use_structural_analysis=True
    )
    
    enhanced_importance = improved_calculator.compute_enhanced_importance(
        model=self.model,
        modules=self.modules,
        channel_trace=channel_trace,
        dataloader=dataloader,  # éœ€è¦ä¼ å…¥
        criterion=criterion,    # éœ€è¦ä¼ å…¥
        device=device
    )
    
    # å°†ç»“æœè½¬æ¢ä¸ºåŸæ ¼å¼
    for layer_name, importance_scores in enhanced_importance.items():
        self.importances[layer_name] = (importance_scores, len(importance_scores))
    '''
    
    print("ğŸ“ é›†æˆä»£ç ç¤ºä¾‹:")
    print(example_code)
    
    print("\nğŸ“‹ é›†æˆæ­¥éª¤:")
    print("1. åœ¨HessianPruner.__init__ä¸­æ·»åŠ improved_calculatorå‚æ•°")
    print("2. åœ¨_compute_hessian_importanceä¸­æ›¿æ¢é‡è¦æ€§è®¡ç®—éƒ¨åˆ†")
    print("3. ä¼ å…¥dataloaderå’Œcriterionå‚æ•°")
    print("4. å¯é€‰ï¼šä½¿ç”¨è‡ªé€‚åº”ä¿ç•™ç­–ç•¥æ›¿æ¢å›ºå®šé˜ˆå€¼")

if __name__ == "__main__":
    integrate_with_hessian_pruner() 