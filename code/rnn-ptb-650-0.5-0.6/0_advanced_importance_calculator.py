#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é«˜çº§ç¥ç»å…ƒé‡è¦æ€§è®¡ç®—å™¨
æä¾›å¤šç§ç²¾ç»†çš„ç¥ç»å…ƒé‡è¦æ€§è¯„ä¼°æ–¹æ³•

ä½œè€…ï¼šAIåŠ©æ‰‹
ç›®çš„ï¼šè§£å†³å½“å‰é‡è¦æ€§è®¡ç®—ç²—ç²’åº¦çš„é—®é¢˜ï¼Œæä¾›æ›´å‡†ç¡®çš„ç¥ç»å…ƒé‡è¦æ€§è¯„ä¼°
"""

import torch
import torch.nn as nn
import numpy as np
import math
from collections import defaultdict
from tqdm import tqdm
import copy
from sklearn.decomposition import PCA
from sklearn.metrics import mutual_info_score
import torch.nn.functional as F

class AdvancedImportanceCalculator:
    """
    é«˜çº§ç¥ç»å…ƒé‡è¦æ€§è®¡ç®—å™¨
    æä¾›å¤šç§ç²¾ç»†çš„é‡è¦æ€§è¯„ä¼°æ–¹æ³•
    """
    
    def __init__(self, model, device="cuda"):
        self.model = model
        self.device = torch.device(device)
        self.activation_hooks = {}
        self.gradient_hooks = {}
        self.layer_activations = defaultdict(list)
        self.layer_gradients = defaultdict(list)
        
        # æ³¨å†Œhooksæ¥æ”¶é›†æ¿€æ´»å’Œæ¢¯åº¦ä¿¡æ¯
        self._register_hooks()
    
    def _register_hooks(self):
        """æ³¨å†Œå‰å‘å’Œåå‘hooksæ¥æ”¶é›†æ¿€æ´»å’Œæ¢¯åº¦ä¿¡æ¯"""
        def make_activation_hook(name):
            def hook(module, input, output):
                if isinstance(output, torch.Tensor):
                    # åªæ”¶é›†å‰Næ‰¹æ•°æ®ï¼Œé¿å…å†…å­˜æº¢å‡º
                    if len(self.layer_activations[name]) < 50:
                        self.layer_activations[name].append(output.detach().cpu())
            return hook
        
        def make_gradient_hook(name):
            def hook(module, grad_input, grad_output):
                if grad_output[0] is not None:
                    if len(self.layer_gradients[name]) < 50:
                        self.layer_gradients[name].append(grad_output[0].detach().cpu())
            return hook
        
        # ä¸ºç›®æ ‡å±‚æ³¨å†Œhooks
        target_layers = ['snn_fc1', 'rnn_fc1', 'snn_fc2', 'rnn_fc2']
        for name, module in self.model.named_parameters():
            layer_name = name.split('.')[0]  # è·å–å±‚å
            if layer_name in target_layers:
                # è·å–å¯¹åº”çš„æ¨¡å—
                module_obj = self.model
                for attr in name.split('.')[:-1]:
                    module_obj = getattr(module_obj, attr)
                
                if layer_name not in self.activation_hooks:
                    self.activation_hooks[layer_name] = module_obj.register_forward_hook(
                        make_activation_hook(layer_name)
                    )
                    self.gradient_hooks[layer_name] = module_obj.register_backward_hook(
                        make_gradient_hook(layer_name)
                    )
    
    def compute_comprehensive_importance(self, dataloader, criterion, methods=['basic', 'activation', 'gradient_flow', 'information_bottleneck', 'cooperative']):
        """
        è®¡ç®—ç»¼åˆç¥ç»å…ƒé‡è¦æ€§
        
        Args:
            dataloader: æ•°æ®åŠ è½½å™¨
            criterion: æŸå¤±å‡½æ•°
            methods: è¦ä½¿ç”¨çš„é‡è¦æ€§è®¡ç®—æ–¹æ³•åˆ—è¡¨
            
        Returns:
            dict: åŒ…å«å„ç§é‡è¦æ€§åˆ†æ•°çš„å­—å…¸
        """
        print(f"\nğŸ”¬ å¼€å§‹è®¡ç®—é«˜çº§ç¥ç»å…ƒé‡è¦æ€§...")
        print(f"ğŸ“‹ ä½¿ç”¨æ–¹æ³•: {methods}")
        
        importance_scores = {}
        
        # 1. åŸºç¡€é‡è¦æ€§ï¼ˆç°æœ‰æ–¹æ³•çš„æ”¹è¿›ç‰ˆï¼‰
        if 'basic' in methods:
            print("ğŸ”¸ è®¡ç®—åŸºç¡€é‡è¦æ€§ï¼ˆæ”¹è¿›ç‰ˆï¼‰...")
            importance_scores['basic'] = self._compute_enhanced_basic_importance()
        
        # 2. åŸºäºæ¿€æ´»çš„é‡è¦æ€§
        if 'activation' in methods:
            print("ğŸ”¸ è®¡ç®—åŸºäºæ¿€æ´»çš„é‡è¦æ€§...")
            importance_scores['activation'] = self._compute_activation_importance(dataloader, criterion)
        
        # 3. æ¢¯åº¦æµé‡è¦æ€§
        if 'gradient_flow' in methods:
            print("ğŸ”¸ è®¡ç®—æ¢¯åº¦æµé‡è¦æ€§...")
            importance_scores['gradient_flow'] = self._compute_gradient_flow_importance(dataloader, criterion)
        
        # 4. ä¿¡æ¯ç“¶é¢ˆé‡è¦æ€§
        if 'information_bottleneck' in methods:
            print("ğŸ”¸ è®¡ç®—ä¿¡æ¯ç“¶é¢ˆé‡è¦æ€§...")
            importance_scores['information_bottleneck'] = self._compute_information_importance(dataloader)
        
        # 5. ååŒé‡è¦æ€§
        if 'cooperative' in methods:
            print("ğŸ”¸ è®¡ç®—ååŒé‡è¦æ€§...")
            importance_scores['cooperative'] = self._compute_cooperative_importance(dataloader, criterion)
        
        # 6. ç»“æ„åŒ–é‡è¦æ€§ï¼ˆè€ƒè™‘ç½‘ç»œç»“æ„ï¼‰
        if 'structural' in methods:
            print("ğŸ”¸ è®¡ç®—ç»“æ„åŒ–é‡è¦æ€§...")
            importance_scores['structural'] = self._compute_structural_importance()
        
        # 7. ç»¼åˆé‡è¦æ€§ï¼ˆå¤šæ–¹æ³•èåˆï¼‰
        print("ğŸ”¸ èåˆå¤šç§é‡è¦æ€§åˆ†æ•°...")
        importance_scores['comprehensive'] = self._fuse_importance_scores(importance_scores)
        
        # æ¸…ç†hookså’Œç¼“å­˜
        self._cleanup_hooks()
        
        return importance_scores
    
    def _compute_enhanced_basic_importance(self):
        """
        å¢å¼ºç‰ˆåŸºç¡€é‡è¦æ€§è®¡ç®—
        æ”¹è¿›åŸæœ‰çš„ Hessian_trace Ã— weight_norm æ–¹æ³•
        """
        basic_importance = {}
        
        # è·å–æƒé‡å‚æ•°
        target_layers = ['snn_fc1', 'rnn_fc1', 'snn_fc2', 'rnn_fc2']
        
        for name, param in self.model.named_parameters():
            layer_name = name.split('.')[0]
            if layer_name in target_layers and 'weight' in name:
                weights = param.data.cpu()
                
                # è®¡ç®—å¤šç§æƒé‡ç»Ÿè®¡é‡
                importance_per_neuron = []
                
                # åŸå§‹ç»´åº¦ï¼š[output_neurons, input_neurons]
                # éå†æ¯ä¸ªè¾“å‡ºç¥ç»å…ƒï¼ˆå¯¹åº”ä¸€è¡Œæƒé‡ï¼‰
                for neuron_idx in range(weights.shape[0]):
                    neuron_weights = weights[neuron_idx]  # è¯¥ç¥ç»å…ƒçš„æƒé‡å‘é‡
                    
                    # 1. L2èŒƒæ•°ï¼ˆåŸæœ‰æ–¹æ³•ï¼‰
                    l2_norm = torch.norm(neuron_weights, p=2).item()
                    
                    # 2. L1èŒƒæ•°ï¼ˆç¨€ç–æ€§æŒ‡æ ‡ï¼‰
                    l1_norm = torch.norm(neuron_weights, p=1).item()
                    
                    # 3. æƒé‡æ–¹å·®ï¼ˆè¡¡é‡æƒé‡åˆ†å¸ƒçš„ç¦»æ•£ç¨‹åº¦ï¼‰
                    weight_var = torch.var(neuron_weights).item()
                    
                    # 4. éé›¶æƒé‡æ¯”ä¾‹ï¼ˆè¡¡é‡è¿æ¥çš„æœ‰æ•ˆæ€§ï¼‰
                    nonzero_ratio = (neuron_weights.abs() > 1e-6).float().mean().item()
                    
                    # 5. æƒé‡ç†µï¼ˆè¡¡é‡ä¿¡æ¯å†…å®¹ï¼‰
                    normalized_weights = F.softmax(neuron_weights.abs(), dim=0)
                    weight_entropy = -(normalized_weights * torch.log(normalized_weights + 1e-8)).sum().item()
                    
                    # 6. æƒé‡çš„æœ‰æ•ˆç§©ï¼ˆé€šè¿‡SVDåˆ†æï¼‰
                    if len(neuron_weights.shape) > 1:
                        U, S, V = torch.svd(neuron_weights.unsqueeze(0))
                        effective_rank = (S > S.max() * 0.01).sum().item()
                    else:
                        effective_rank = 1.0
                    
                    # ç»¼åˆé‡è¦æ€§è¯„åˆ†
                    enhanced_importance = (
                        l2_norm * 0.3 +                    # æƒé‡å¤§å°
                        weight_var * 0.2 +                 # æƒé‡å¤šæ ·æ€§
                        nonzero_ratio * 0.2 +              # è¿æ¥æœ‰æ•ˆæ€§
                        weight_entropy * 0.2 +             # ä¿¡æ¯å†…å®¹
                        effective_rank * 0.1               # è¡¨ç¤ºèƒ½åŠ›
                    )
                    
                    importance_per_neuron.append(enhanced_importance)
                
                basic_importance[layer_name] = importance_per_neuron
        
        return basic_importance
    
    def _compute_activation_importance(self, dataloader, criterion):
        """
        åŸºäºæ¿€æ´»æ¨¡å¼çš„é‡è¦æ€§è®¡ç®—
        è€ƒè™‘ç¥ç»å…ƒçš„å®é™…æ¿€æ´»æƒ…å†µå’Œä¿¡æ¯ä¼ é€’èƒ½åŠ›
        """
        print("   ğŸ“Š æ”¶é›†æ¿€æ´»æ•°æ®...")
        
        # æ¸…ç©ºä¹‹å‰çš„æ¿€æ´»è®°å½•
        self.layer_activations.clear()
        
        self.model.eval()
        activation_importance = {}
        
        # æ”¶é›†æ¿€æ´»æ•°æ®
        sample_count = 0
        max_samples = 200  # é™åˆ¶æ ·æœ¬æ•°é‡é¿å…å†…å­˜é—®é¢˜
        
        with torch.no_grad():
            for batch_idx, (data, targets) in enumerate(dataloader):
                if sample_count >= max_samples:
                    break
                
                data, targets = data.to(self.device), targets.to(self.device)
                
                # åˆå§‹åŒ–éšè—çŠ¶æ€
                hidden = self.model.init_hidden(data.size(1))
                
                # å‰å‘ä¼ æ’­æ”¶é›†æ¿€æ´»
                output, hidden = self.model(data, hidden)
                
                sample_count += data.size(1)  # batch_size
        
        print(f"   ğŸ“ˆ åˆ†æ {len(self.layer_activations)} å±‚çš„æ¿€æ´»æ¨¡å¼...")
        
        # åˆ†ææ¯å±‚çš„æ¿€æ´»é‡è¦æ€§
        for layer_name, activations in self.layer_activations.items():
            if not activations:
                continue
                
            # å°†æ‰€æœ‰æ‰¹æ¬¡çš„æ¿€æ´»è¿æ¥èµ·æ¥
            all_activations = torch.cat(activations, dim=0)  # [total_samples, neurons]
            
            importance_per_neuron = []
            
            for neuron_idx in range(all_activations.shape[-1]):
                neuron_activations = all_activations[..., neuron_idx]
                
                # 1. æ¿€æ´»é¢‘ç‡ï¼ˆç¥ç»å…ƒæœ‰å¤šé¢‘ç¹è¢«æ¿€æ´»ï¼‰
                activation_freq = (neuron_activations > 0).float().mean().item()
                
                # 2. æ¿€æ´»å¼ºåº¦ï¼ˆå¹³å‡æ¿€æ´»å€¼ï¼‰
                activation_magnitude = neuron_activations.abs().mean().item()
                
                # 3. æ¿€æ´»æ–¹å·®ï¼ˆæ¿€æ´»çš„å˜åŒ–ç¨‹åº¦ï¼‰
                activation_variance = neuron_activations.var().item()
                
                # 4. æ¿€æ´»ç†µï¼ˆæ¿€æ´»æ¨¡å¼çš„å¤æ‚æ€§ï¼‰
                # å°†æ¿€æ´»å€¼åˆ†ä¸ºä¸åŒåŒºé—´è®¡ç®—ç†µ
                hist, _ = np.histogram(neuron_activations.numpy(), bins=10, density=True)
                hist = hist + 1e-8  # é¿å…log(0)
                activation_entropy = -(hist * np.log(hist)).sum()
                
                # 5. æ¿€æ´»çš„åˆ¤åˆ«èƒ½åŠ›ï¼ˆé€šè¿‡ä¸åŒç±»åˆ«çš„æ¿€æ´»å·®å¼‚è¡¡é‡ï¼‰
                # è¿™é‡Œç®€åŒ–ä¸ºæ¿€æ´»å€¼çš„åŠ¨æ€èŒƒå›´
                activation_range = neuron_activations.max().item() - neuron_activations.min().item()
                
                # ç»¼åˆæ¿€æ´»é‡è¦æ€§
                activation_importance_score = (
                    activation_freq * 0.2 +           # æ¿€æ´»é¢‘ç‡
                    activation_magnitude * 0.3 +      # æ¿€æ´»å¼ºåº¦
                    activation_variance * 0.2 +       # æ¿€æ´»å¤šæ ·æ€§
                    activation_entropy * 0.2 +        # æ¿€æ´»å¤æ‚æ€§
                    activation_range * 0.1            # åˆ¤åˆ«èƒ½åŠ›
                )
                
                importance_per_neuron.append(activation_importance_score)
            
            activation_importance[layer_name] = importance_per_neuron
        
        return activation_importance
    
    def _compute_gradient_flow_importance(self, dataloader, criterion):
        """
        åŸºäºæ¢¯åº¦æµçš„é‡è¦æ€§è®¡ç®—
        åˆ†ææ¢¯åº¦åœ¨ç½‘ç»œä¸­çš„ä¼ æ’­æƒ…å†µ
        """
        print("   ğŸŒŠ åˆ†ææ¢¯åº¦æµæ¨¡å¼...")
        
        # æ¸…ç©ºæ¢¯åº¦è®°å½•
        self.layer_gradients.clear()
        
        self.model.train()
        gradient_importance = {}
        
        # æ”¶é›†æ¢¯åº¦æ•°æ®
        sample_count = 0
        max_samples = 100  # æ¢¯åº¦è®¡ç®—æ›´æ¶ˆè€—èµ„æºï¼Œé™åˆ¶æ›´å°‘æ ·æœ¬
        
        for batch_idx, (data, targets) in enumerate(dataloader):
            if sample_count >= max_samples:
                break
            
            data, targets = data.to(self.device), targets.to(self.device)
            
            # å‰å‘ä¼ æ’­
            hidden = self.model.init_hidden(data.size(1))
            output, hidden = self.model(data, hidden)
            
            # è®¡ç®—æŸå¤±å¹¶åå‘ä¼ æ’­
            loss = criterion(output.view(-1, output.size(-1)), targets.view(-1))
            
            self.model.zero_grad()
            loss.backward()
            
            sample_count += data.size(1)
        
        print(f"   ğŸ“‰ åˆ†æ {len(self.layer_gradients)} å±‚çš„æ¢¯åº¦ç‰¹å¾...")
        
        # åˆ†ææ¯å±‚çš„æ¢¯åº¦é‡è¦æ€§
        for layer_name, gradients in self.layer_gradients.items():
            if not gradients:
                continue
            
            # å°†æ‰€æœ‰æ‰¹æ¬¡çš„æ¢¯åº¦è¿æ¥èµ·æ¥
            all_gradients = torch.cat(gradients, dim=0)
            
            importance_per_neuron = []
            
            for neuron_idx in range(all_gradients.shape[-1]):
                neuron_gradients = all_gradients[..., neuron_idx]
                
                # 1. æ¢¯åº¦å¹…å€¼ï¼ˆæ¢¯åº¦çš„å¹³å‡å¤§å°ï¼‰
                gradient_magnitude = neuron_gradients.abs().mean().item()
                
                # 2. æ¢¯åº¦ç¨³å®šæ€§ï¼ˆæ¢¯åº¦çš„ä¸€è‡´æ€§ï¼‰
                gradient_stability = 1.0 / (neuron_gradients.var().item() + 1e-8)
                
                # 3. æ¢¯åº¦ä¿¡å™ªæ¯”
                signal = neuron_gradients.abs().mean().item()
                noise = neuron_gradients.std().item()
                snr = signal / (noise + 1e-8)
                
                # 4. æ¢¯åº¦æ–¹å‘ä¸€è‡´æ€§
                sign_consistency = (neuron_gradients > 0).float().mean().item()
                sign_consistency = max(sign_consistency, 1 - sign_consistency)  # å–æ›´ä¸€è‡´çš„æ–¹å‘
                
                # ç»¼åˆæ¢¯åº¦é‡è¦æ€§
                gradient_importance_score = (
                    gradient_magnitude * 0.4 +        # æ¢¯åº¦å¤§å°
                    gradient_stability * 0.2 +        # æ¢¯åº¦ç¨³å®šæ€§
                    snr * 0.2 +                       # ä¿¡å™ªæ¯”
                    sign_consistency * 0.2            # æ–¹å‘ä¸€è‡´æ€§
                )
                
                importance_per_neuron.append(gradient_importance_score)
            
            gradient_importance[layer_name] = importance_per_neuron
        
        return gradient_importance
    
    def _compute_information_importance(self, dataloader):
        """
        åŸºäºä¿¡æ¯ç“¶é¢ˆç†è®ºçš„é‡è¦æ€§è®¡ç®—
        åˆ†æç¥ç»å…ƒçš„ä¿¡æ¯ä¼ é€’å’Œå‹ç¼©èƒ½åŠ›
        """
        print("   ğŸ“¡ è®¡ç®—ä¿¡æ¯ç“¶é¢ˆé‡è¦æ€§...")
        
        # è¿™æ˜¯ä¸€ä¸ªç®€åŒ–ç‰ˆçš„ä¿¡æ¯ç“¶é¢ˆåˆ†æ
        # å®Œæ•´ç‰ˆéœ€è¦ä¼°è®¡äº’ä¿¡æ¯ï¼Œè®¡ç®—é‡å¾ˆå¤§
        
        information_importance = {}
        
        # æ”¶é›†è¾“å…¥è¾“å‡ºæ•°æ®ç”¨äºä¿¡æ¯åˆ†æ
        input_data = []
        output_data = {}
        
        self.model.eval()
        sample_count = 0
        max_samples = 150
        
        with torch.no_grad():
            for batch_idx, (data, targets) in enumerate(dataloader):
                if sample_count >= max_samples:
                    break
                
                data = data.to(self.device)
                input_data.append(data.cpu())
                
                hidden = self.model.init_hidden(data.size(1))
                output, hidden = self.model(data, hidden)
                
                sample_count += data.size(1)
        
        # åˆ†ææ¯å±‚çš„ä¿¡æ¯ä¼ é€’èƒ½åŠ›
        for layer_name, activations in self.layer_activations.items():
            if not activations:
                continue
            
            all_activations = torch.cat(activations, dim=0)
            importance_per_neuron = []
            
            for neuron_idx in range(all_activations.shape[-1]):
                neuron_activations = all_activations[..., neuron_idx].numpy()
                
                # 1. ä¿¡æ¯ç†µï¼ˆç¥ç»å…ƒè¾“å‡ºçš„ä¿¡æ¯é‡ï¼‰
                # ç¦»æ•£åŒ–æ¿€æ´»å€¼æ¥è®¡ç®—ç†µ
                hist, _ = np.histogram(neuron_activations, bins=20, density=True)
                hist = hist + 1e-8
                information_entropy = -(hist * np.log(hist)).sum()
                
                # 2. ä¿¡æ¯ä¼ é€’æ•ˆç‡ï¼ˆåŸºäºæ¿€æ´»å€¼çš„å˜åŒ–ï¼‰
                if len(neuron_activations) > 1:
                    # è®¡ç®—ç›¸é‚»æ—¶åˆ»æ¿€æ´»å€¼çš„ç›¸å…³æ€§
                    autocorr = np.corrcoef(neuron_activations[:-1], neuron_activations[1:])[0, 1]
                    if np.isnan(autocorr):
                        autocorr = 0
                    information_transfer = 1 - abs(autocorr)  # ä½ç›¸å…³æ€§æ„å‘³ç€é«˜ä¿¡æ¯ä¼ é€’
                else:
                    information_transfer = 0.5
                
                # 3. ä¿¡æ¯å‹ç¼©èƒ½åŠ›ï¼ˆé€šè¿‡PCAåˆ†æï¼‰
                if neuron_activations.var() > 1e-6:
                    # ç®€åŒ–çš„ä¿¡æ¯å‹ç¼©æŒ‡æ ‡ï¼šåŸºäºæ¿€æ´»å€¼çš„åŠ¨æ€èŒƒå›´
                    dynamic_range = neuron_activations.max() - neuron_activations.min()
                    compression_ability = dynamic_range / (neuron_activations.std() + 1e-8)
                else:
                    compression_ability = 0
                
                # ç»¼åˆä¿¡æ¯é‡è¦æ€§
                information_importance_score = (
                    information_entropy * 0.4 +           # ä¿¡æ¯é‡
                    information_transfer * 0.3 +          # ä¼ é€’æ•ˆç‡
                    compression_ability * 0.3             # å‹ç¼©èƒ½åŠ›
                )
                
                importance_per_neuron.append(information_importance_score)
            
            information_importance[layer_name] = importance_per_neuron
        
        return information_importance
    
    def _compute_cooperative_importance(self, dataloader, criterion):
        """
        ååŒé‡è¦æ€§è®¡ç®—
        è€ƒè™‘ç¥ç»å…ƒä¹‹é—´çš„ç›¸äº’ä½œç”¨å’ŒååŒæ•ˆåº”
        """
        print("   ğŸ¤ åˆ†æç¥ç»å…ƒååŒæ•ˆåº”...")
        
        cooperative_importance = {}
        
        # ä½¿ç”¨æ¿€æ´»æ•°æ®åˆ†æç¥ç»å…ƒé—´ç›¸å…³æ€§
        for layer_name, activations in self.layer_activations.items():
            if not activations:
                continue
            
            all_activations = torch.cat(activations, dim=0)  # [samples, neurons]
            num_neurons = all_activations.shape[-1]
            
            if num_neurons < 2:
                cooperative_importance[layer_name] = [0.5] * num_neurons
                continue
            
            # è®¡ç®—ç¥ç»å…ƒé—´çš„ç›¸å…³çŸ©é˜µ
            activations_np = all_activations.numpy()
            correlation_matrix = np.corrcoef(activations_np.T)
            
            # å¤„ç†NaNå€¼
            correlation_matrix = np.nan_to_num(correlation_matrix, 0)
            
            importance_per_neuron = []
            
            for neuron_idx in range(num_neurons):
                # 1. è¿æ¥å¼ºåº¦ï¼ˆä¸å…¶ä»–ç¥ç»å…ƒçš„å¹³å‡ç›¸å…³æ€§ï¼‰
                neuron_correlations = correlation_matrix[neuron_idx]
                connection_strength = np.abs(neuron_correlations).mean()
                
                # 2. ç‹¬ç‰¹æ€§ï¼ˆä½å†—ä½™æ€§ï¼‰
                # è®¡ç®—è¯¥ç¥ç»å…ƒä¸å…¶ä»–ç¥ç»å…ƒçš„æœ€å¤§ç›¸å…³æ€§
                max_correlation = np.max(np.abs(neuron_correlations[neuron_correlations != 1.0]))
                uniqueness = 1 - max_correlation
                
                # 3. ä¸­å¿ƒæ€§ï¼ˆç½‘ç»œä¸­çš„é‡è¦ä½ç½®ï¼‰
                # åŸºäºç›¸å…³æ€§çš„åº¦ä¸­å¿ƒæ€§
                degree_centrality = np.sum(np.abs(neuron_correlations) > 0.1)
                degree_centrality = degree_centrality / (num_neurons - 1)
                
                # 4. ååŒæ•ˆåº”ï¼ˆä¸å¤šä¸ªç¥ç»å…ƒçš„åä½œèƒ½åŠ›ï¼‰
                # è®¡ç®—ä¸‰å…ƒç»„ååŒæ•ˆåº”çš„ç®€åŒ–ç‰ˆæœ¬
                cooperation_count = 0
                for i in range(num_neurons):
                    for j in range(i+1, num_neurons):
                        if i != neuron_idx and j != neuron_idx:
                            # æ£€æŸ¥ä¸‰ä¸ªç¥ç»å…ƒä¹‹é—´çš„ååŒæ¨¡å¼
                            corr_ij = correlation_matrix[i, j]
                            corr_ni = correlation_matrix[neuron_idx, i]
                            corr_nj = correlation_matrix[neuron_idx, j]
                            
                            # å¦‚æœä¸‰è€…éƒ½æœ‰ä¸€å®šç›¸å…³æ€§ï¼Œè®¤ä¸ºå­˜åœ¨ååŒæ•ˆåº”
                            if abs(corr_ij) > 0.1 and abs(corr_ni) > 0.1 and abs(corr_nj) > 0.1:
                                cooperation_count += 1
                
                cooperation_ability = cooperation_count / max(1, num_neurons * (num_neurons - 1) // 2)
                
                # ç»¼åˆååŒé‡è¦æ€§
                cooperative_importance_score = (
                    connection_strength * 0.3 +       # è¿æ¥å¼ºåº¦
                    uniqueness * 0.3 +               # ç‹¬ç‰¹æ€§
                    degree_centrality * 0.2 +        # ä¸­å¿ƒæ€§
                    cooperation_ability * 0.2        # ååŒèƒ½åŠ›
                )
                
                importance_per_neuron.append(cooperative_importance_score)
            
            cooperative_importance[layer_name] = importance_per_neuron
        
        return cooperative_importance
    
    def _compute_structural_importance(self):
        """
        ç»“æ„åŒ–é‡è¦æ€§è®¡ç®—
        è€ƒè™‘ç½‘ç»œçš„å±‚æ¬¡ç»“æ„å’Œä¿¡æ¯æµ
        """
        print("   ğŸ—ï¸ åˆ†æç½‘ç»œç»“æ„é‡è¦æ€§...")
        
        structural_importance = {}
        
        # ç®€åŒ–çš„ç»“æ„é‡è¦æ€§ï¼šåŸºäºå±‚çš„ä½ç½®å’Œè¿æ¥æ¨¡å¼
        layer_order = ['snn_fc1', 'rnn_fc1', 'snn_fc2', 'rnn_fc2']
        
        for layer_idx, layer_name in enumerate(layer_order):
            # æ£€æŸ¥è¯¥å±‚æ˜¯å¦å­˜åœ¨
            layer_found = False
            for name, param in self.model.named_parameters():
                if layer_name in name and 'weight' in name:
                    layer_found = True
                    weights = param.data.cpu()
                    num_neurons = weights.shape[0]
                    
                    importance_per_neuron = []
                    
                    for neuron_idx in range(num_neurons):
                        # 1. å±‚ä½ç½®é‡è¦æ€§ï¼ˆæ—©æœŸå±‚å’Œæ™šæœŸå±‚æ›´é‡è¦ï¼‰
                        if layer_idx <= 1:  # ç¬¬ä¸€å±‚
                            position_importance = 0.8
                        else:  # ç¬¬äºŒå±‚
                            position_importance = 0.9
                        
                        # 2. æ‰‡å‡ºé‡è¦æ€§ï¼ˆè¯¥ç¥ç»å…ƒè¿æ¥åˆ°ä¸‹ä¸€å±‚çš„ç¨‹åº¦ï¼‰
                        # åœ¨æ··åˆç½‘ç»œä¸­ï¼Œæ¯ä¸ªç¥ç»å…ƒéƒ½ä¼šè¿æ¥åˆ°ä¸‹ä¸€å±‚çš„æ‰€æœ‰ç¥ç»å…ƒ
                        fanout_importance = 1.0  # ç®€åŒ–å‡è®¾
                        
                        # 3. è·¯å¾„é‡è¦æ€§ï¼ˆè¯¥ç¥ç»å…ƒåœ¨ä¿¡æ¯ä¼ é€’è·¯å¾„ä¸­çš„é‡è¦æ€§ï¼‰
                        # RNNå’ŒSNNçš„cascadeè¿æ¥æ¨¡å¼
                        if 'rnn' in layer_name:
                            path_importance = 0.8  # RNNæä¾›è¿ç»­ä¿¡æ¯
                        else:
                            path_importance = 0.7  # SNNæä¾›ç¦»æ•£ä¿¡æ¯
                        
                        # ç»¼åˆç»“æ„é‡è¦æ€§
                        structural_importance_score = (
                            position_importance * 0.4 +
                            fanout_importance * 0.3 +
                            path_importance * 0.3
                        )
                        
                        importance_per_neuron.append(structural_importance_score)
                    
                    structural_importance[layer_name] = importance_per_neuron
                    break
            
            if not layer_found:
                print(f"   âš ï¸ å±‚ {layer_name} æœªæ‰¾åˆ°")
        
        return structural_importance
    
    def _fuse_importance_scores(self, importance_scores):
        """
        èåˆå¤šç§é‡è¦æ€§åˆ†æ•°
        ä½¿ç”¨åŠ æƒå¹³å‡å’Œå½’ä¸€åŒ–
        """
        print("   ğŸ”„ èåˆå¤šç§é‡è¦æ€§åˆ†æ•°...")
        
        # å®šä¹‰å„æ–¹æ³•çš„æƒé‡
        method_weights = {
            'basic': 0.25,              # åŸºç¡€é‡è¦æ€§
            'activation': 0.25,         # æ¿€æ´»é‡è¦æ€§
            'gradient_flow': 0.2,       # æ¢¯åº¦æµé‡è¦æ€§
            'information_bottleneck': 0.15,  # ä¿¡æ¯ç“¶é¢ˆé‡è¦æ€§
            'cooperative': 0.1,         # ååŒé‡è¦æ€§
            'structural': 0.05          # ç»“æ„é‡è¦æ€§
        }
        
        fused_importance = {}
        
        # è·å–æ‰€æœ‰å±‚çš„åç§°
        all_layers = set()
        for method, scores in importance_scores.items():
            all_layers.update(scores.keys())
        
        for layer_name in all_layers:
            # æ”¶é›†è¯¥å±‚æ‰€æœ‰æ–¹æ³•çš„åˆ†æ•°
            layer_scores = {}
            max_neurons = 0
            
            for method, scores in importance_scores.items():
                if layer_name in scores:
                    layer_scores[method] = scores[layer_name]
                    max_neurons = max(max_neurons, len(scores[layer_name]))
            
            if max_neurons == 0:
                continue
            
            # å½’ä¸€åŒ–å„æ–¹æ³•çš„åˆ†æ•°åˆ°[0,1]èŒƒå›´
            normalized_scores = {}
            for method, scores in layer_scores.items():
                scores_array = np.array(scores)
                if scores_array.max() > scores_array.min():
                    normalized = (scores_array - scores_array.min()) / (scores_array.max() - scores_array.min())
                else:
                    normalized = np.ones_like(scores_array) * 0.5
                normalized_scores[method] = normalized
            
            # åŠ æƒèåˆ
            fused_scores = np.zeros(max_neurons)
            total_weight = 0
            
            for method, weight in method_weights.items():
                if method in normalized_scores:
                    fused_scores += normalized_scores[method] * weight
                    total_weight += weight
            
            if total_weight > 0:
                fused_scores /= total_weight
            
            fused_importance[layer_name] = fused_scores.tolist()
        
        return fused_importance
    
    def _cleanup_hooks(self):
        """æ¸…ç†æ³¨å†Œçš„hooks"""
        for hook in self.activation_hooks.values():
            hook.remove()
        for hook in self.gradient_hooks.values():
            hook.remove()
        
        self.activation_hooks.clear()
        self.gradient_hooks.clear()
        self.layer_activations.clear()
        self.layer_gradients.clear()
    
    def analyze_importance_quality(self, importance_scores, ground_truth_method='comprehensive'):
        """
        åˆ†æé‡è¦æ€§è®¡ç®—çš„è´¨é‡
        é€šè¿‡å¤šç§æŒ‡æ ‡è¯„ä¼°é‡è¦æ€§åˆ†æ•°çš„åˆç†æ€§
        """
        print(f"\nğŸ” åˆ†æé‡è¦æ€§è®¡ç®—è´¨é‡...")
        
        quality_metrics = {}
        
        if ground_truth_method not in importance_scores:
            print(f"âš ï¸ å‚è€ƒæ–¹æ³• {ground_truth_method} ä¸å­˜åœ¨ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªæ–¹æ³•ä½œä¸ºå‚è€ƒ")
            ground_truth_method = list(importance_scores.keys())[0]
        
        reference_scores = importance_scores[ground_truth_method]
        
        for method_name, method_scores in importance_scores.items():
            if method_name == ground_truth_method:
                continue
            
            method_quality = {}
            
            for layer_name in reference_scores.keys():
                if layer_name in method_scores:
                    ref_scores = np.array(reference_scores[layer_name])
                    method_score_array = np.array(method_scores[layer_name])
                    
                    # 1. ç›¸å…³æ€§åˆ†æ
                    correlation = np.corrcoef(ref_scores, method_score_array)[0, 1]
                    if np.isnan(correlation):
                        correlation = 0
                    
                    # 2. æ’åºä¸€è‡´æ€§ï¼ˆSpearmanç›¸å…³ï¼‰
                    from scipy.stats import spearmanr
                    rank_correlation, _ = spearmanr(ref_scores, method_score_array)
                    if np.isnan(rank_correlation):
                        rank_correlation = 0
                    
                    # 3. åˆ†å¸ƒç›¸ä¼¼æ€§ï¼ˆKLæ•£åº¦ï¼‰
                    ref_normalized = ref_scores / (ref_scores.sum() + 1e-8)
                    method_normalized = method_score_array / (method_score_array.sum() + 1e-8)
                    
                    # è®¡ç®—KLæ•£åº¦
                    kl_div = np.sum(ref_normalized * np.log((ref_normalized + 1e-8) / (method_normalized + 1e-8)))
                    
                    method_quality[layer_name] = {
                        'correlation': correlation,
                        'rank_correlation': rank_correlation,
                        'kl_divergence': kl_div,
                        'quality_score': (correlation + rank_correlation) / 2 - kl_div * 0.1
                    }
            
            quality_metrics[method_name] = method_quality
        
        # æ‰“å°è´¨é‡åˆ†æç»“æœ
        for method_name, method_quality in quality_metrics.items():
            print(f"\nğŸ“Š {method_name} æ–¹æ³•è´¨é‡åˆ†æ:")
            for layer_name, metrics in method_quality.items():
                print(f"  {layer_name}:")
                print(f"    ç›¸å…³æ€§: {metrics['correlation']:.3f}")
                print(f"    æ’åºç›¸å…³æ€§: {metrics['rank_correlation']:.3f}")
                print(f"    KLæ•£åº¦: {metrics['kl_divergence']:.3f}")
                print(f"    ç»¼åˆè´¨é‡åˆ†æ•°: {metrics['quality_score']:.3f}")
        
        return quality_metrics 